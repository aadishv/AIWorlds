Following is a description of an issue I am having. Use the following logic when reasoning about the issues:
**Part 1**
1) Attempt to identify the cause.
2) If you are able to identify the cause, proceed to Part 2.
3) If you are unable to identify the cause or if there are multiple options, instruct the user to gain more information. Give the user a step-by-step guide on how to obtain the information. Methods of obtaining information may include pasting in the contents of a particular file, running a terminal command, linking to documentation, etc. After the user responds with the extra information, go back to step 1.
**Part 2**
At this step, you now know the cause.
a) Attempt to identify a solution.
b) If you are able to identify the solution, proceed to Part 3.
c) If you are unable to identify the solution or if there are multiple options, instruct the user to gain more information. Give the user a step-by-step guide on how to obtain the information. Methods of obtaining information may include pasting in the contents of a particular file, running a terminal command, linking to documentation, etc. After the user responds with the extra information, go back to step 1.
**Part 3**
At this step, you know the solution.
Give the user a step-by-step guide on how to resolve the issue. Each step can come in one of the following formats:
1) Exact edits to files. Provide the name of the file (with path if there is ambiguity), the part to search for, and the part to replace.
2) Terminal commands to run. Provide the directory to run the command in (`.` suffices usually) and the exact command to run. Also instruct the user about possible errors and tell the user to come back if such an error occurs. If the user responds with an error when they ran the terminal command, proceed back to Part 1.
3) Request permission to the system in order to use any other kind of steps.

Make sure you verbalize your thinking, explaining which step you are on (i.e., Part 2a) and what your conclusion is before moving on to the next step. Double-check your work and do not be afraid to ask for more information. Verbalize your thinking inside <think> blocks, not the final response. When asking the user for extra information, provide only one action at a time (i.e., only ask for the contents of file, or only provide one terminal command). The issue follows.




Old code for reference:

AIWorlds/comp2
```md AIWorlds/comp2/README.md
Example thing the jetson sends to the brain:
```json
{
  "pose": {
    "x": 0,
    "y": 0,
    "theta": 0
  },
  "stuff": [
    {
      "x": 52.625,
      "y": 324.375,
      "width": 33.3125,
      "height": 32.15625,
      "class": "blue",
      "depth": 1.5737864825178338,
      "confidence": 0.6214909553527832
    }
  ],
  "flag": ""
}
```
```

__pycache__

```py AIWorlds/comp2/camera.py
try:
    import pyrealsense2 as rs
    _ = rs.pipeline()
except AttributeError:
    import pyrealsense2.pyrealsense2 as rs
    _ = rs.pipeline()
except Exception:
    print("Error importing pyrealsense2")
    exit(1)
import cv2
import numpy as np

# from highstakes.py, JetsonExample


class Camera:
    def __init__(self):
        self.pipeline = rs.pipeline()  # Initialize RealSense pipeline
        self.config = rs.config()
        # Enable depth stream at 640x480 in z16 encoding at 30fps
        self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        # Enable color stream at 640x480 in rgb8 encoding at 30fps
        self.config.enable_stream(
            rs.stream.color, 640, 480, rs.format.bgr8, 30)

    def start(self):
        self.profile = self.pipeline.start(self.config)  # Start the pipeline
        # Obtain depth sensor and calculate depth scale
        depth_sensor = self.profile.get_device().first_depth_sensor()
        self.depth_scale = depth_sensor.get_depth_scale()

    def get_frames(self):
        return self.pipeline.wait_for_frames()

    def stop(self):
        self.pipeline.stop()  # Stop the pipeline when finished


class Processing:
    # Class to handle camera data processing, preparing for inference, and running inference on camera image.
    def __init__(self, depth_scale):
        self.depth_scale = depth_scale
        self.align_to = rs.stream.color
        self.align = rs.align(self.align_to)
        self.HUE = 1.2
        self.SATURATION = 1.2
        self.VALUE = 0.8

    # checked
    def process_image(self, image):
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        hsv[..., 0] = hsv[..., 0] + self.HUE
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * self.SATURATION, 0, 255)
        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * self.VALUE, 0, 255)
        return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)

    # checked
    def updateHSV(self, newHSV):
        self.HUE = newHSV.h

        if (self.SATURATION >= 0):
            self.SATURATION = 1 + (newHSV.s) / 100
        else:
            self.SATURATION = (100 - abs(newHSV.s)) / 100

        if (self.VALUE >= 0):
            self.VALUE = 1 + (newHSV.v) / 100
        else:
            self.VALUE = (100 - abs(newHSV.v)) / 100

    # checked
    def get_depth(self, detection, depth_img):
        try:
            height = detection["height"]
            width = detection["width"]
            top = max(detection["y"] - height / 2, 0)
            bottom = min(detection["y"] + height / 2, depth_img.shape[0])
            left = max(detection["x"] - width / 2, 0)
            right = min(detection["x"] + width / 2, depth_img.shape[1])
            depth_img = depth_img[int(top):int(
                bottom), int(left):int(right)].astype(float).flatten()
            p = np.percentile(depth_img[depth_img > 0], 10) * self.depth_scale
            return p
        except Exception:
            return -1

    # checked
    def align_frames(self, frames):
        # Align depth frames to color frames
        aligned_frames = self.align.process(frames)
        # Get the aligned frames and validate them
        self.depth_frame_aligned = aligned_frames.get_depth_frame()
        self.color_frame_aligned = aligned_frames.get_color_frame()

        if not self.depth_frame_aligned or not self.color_frame_aligned:
            self.depth_frame_aligned = None
            self.color_frame_aligned = None

    # checked
    def process_frames(self, frames):
        # Align frames and extract color and depth images
        # Apply a color map to the depth image
        self.align_frames(frames)
        depth_image = np.asanyarray(self.depth_frame_aligned.get_data())
        # feed-forward factor, since it is always 3% off
        depth_image = depth_image / 1.03
        color_image = np.asanyarray(self.color_frame_aligned.get_data())
        # apply color correction to image
        color_image = self.process_image(color_image)
        depthImage = cv2.normalize(
            depth_image, None, alpha=0.01, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
        depth_map = cv2.applyColorMap(depthImage, cv2.COLORMAP_JET)

        return depth_image, color_image, depth_map
```

```py AIWorlds/comp2/comms.py
import threading
import sys
import serial
from serial.tools import list_ports


class EventDrivenSerial:
    def __init__(self,
                 port,
                 baudrate,
                 callback,
                 eol=b'\n',
                 timeout=1.0,
                 reconnect_delay=5.0,
                 log_filename='serial_log.txt'):
        """
        :param port:            Serial port name, e.g. 'COM3' or '/dev/ttyUSB0'
        :param baudrate:        Baud rate, e.g. 9600
        :param callback:        Function f(line:str) -> Optional[str]
                                Called on each received line (without newline).
                                If it returns a string, that string (with newline)
                                is written back to the serial port.
        :param eol:             Byte sequence for line endings (default b'\\n')
        :param timeout:         Serial read timeout in seconds
        :param reconnect_delay: Seconds to wait before attempting reconnection after an error
        :param log_filename:    File to which we append all RX/TX traffic
        """
        self.port = port
        self.baudrate = baudrate
        self.callback = callback
        self.eol = eol
        self.timeout = timeout
        self.reconnect_delay = reconnect_delay

        self._ser = None
        self._lock = threading.Lock()       # protects serial writes
        self._log_lock = threading.Lock()   # protects log file writes
        self._stop_event = threading.Event()
        self._thread = None

        # Open the log file in append mode once
        self._log_file = open(log_filename, 'a', buffering=1, encoding='utf-8')
        # buffering=1 means line-buffered. encoding=utf-8 so we're safe with unicode.

    def _log(self, direction, line):
        """Internal helper to write a timestamped RX/TX line to the log."""
        from datetime import datetime
        timestamp = datetime.now().isoformat(sep=' ', timespec='milliseconds')
        entry = f"{timestamp} {direction}: {line}\n"
        with self._log_lock:
            self._log_file.write(entry)

    def _connect(self):
        """Attempts to connect to the serial port."""
        if self._ser and self._ser.is_open:
            return True
        try:
            self._close_serial()
            temp_timeout = min(self.timeout, 1.0)
            self._ser = serial.Serial(self.port, self.baudrate,
                                      timeout=temp_timeout)
            self._ser.timeout = self.timeout
            return True
        except serial.SerialException:
            self._ser = None
            return False
        except Exception:
            self._ser = None
            return False

    def _close_serial(self):
        if self._ser and self._ser.is_open:
            try:
                self._ser.close()
            except Exception as e:
                print(
                    f"[EventDrivenSerial] Error closing port {self.port}: {e}")
        self._ser = None

    def start(self):
        if self._thread and self._thread.is_alive():
            return
        self._stop_event.clear()
        self._thread = threading.Thread(target=self.serial_worker,
                                        daemon=True)
        self._thread.start()

    def stop(self):
        if not self._thread or not self._thread.is_alive():
            return
        self._stop_event.set()

    def _inner_loop(self):
        while not self._stop_event.is_set():
            raw = self._ser.readline()
            if self._stop_event.is_set():
                break
            if not raw:
                continue

            # RX: decode + strip
            line = raw.decode('utf-8', errors='replace').rstrip('\r\n')
            self._log('RX', line)

            # run callback
            try:
                response = self.callback(line)
            except Exception:
                response = None

            if isinstance(response, str):
                # TX: strip any stray newlines then re-append your eol
                out_line = response.rstrip('\r\n')
                self._log('TX', out_line)

                out = out_line.encode('utf-8') + self.eol
                with self._lock:
                    if not (self._ser and self._ser.is_open):
                        raise serial.SerialException(
                            "Lost connection before write")
                    self._ser.write(out)

            # last_time = time.perf_counter()

    def serial_worker(self):
        while not self._stop_event.is_set():
            # if not connected, try to reconnect
            if not (self._ser and self._ser.is_open):
                if not self._connect():
                    if self._stop_event.wait(self.reconnect_delay):
                        break
                    continue

            try:
                self._inner_loop()

            except serial.SerialException:
                self._close_serial()
                if self._stop_event.wait(1.0):
                    break
            except Exception as e:
                print(f"[EventDrivenSerial] Unexpected error: {e}")
                self._close_serial()
                if self._stop_event.wait(self.reconnect_delay):
                    break

        # cleanup
        self._close_serial()
        # close the log file
        with self._log_lock:
            self._log_file.close()

# helper functions unchanged


def list_serial_ports():
    ports = list_ports.comports()
    return [p.device for p in ports]


def prompt_user_for_port(ports):
    print("Available serial ports:")
    for idx, p in enumerate(ports, 1):
        print(f"  {idx}: {p}")
    choice = input(f"Select port [1-{len(ports)}]: ").strip()
    try:
        i = int(choice) - 1
        if i < 0 or i >= len(ports):
            raise ValueError
    except ValueError:
        print("Invalid selection. Exiting.")
        sys.exit(1)
    return ports[i]
```

```py AIWorlds/comp2/eval.py
import os
import datetime
import time
import cv2

from inference import InferenceEngine

def run_eval(process_image) -> str:
    IMAGES = [
        "DSC05527_JPG.rf.1dc38151cc9cb5b70ebc3f3c8f2c9061.jpg",
        "DSC05527_JPG.rf.857f05dd031761433f006fcff38fd7d6.jpg",
        "DSC05527_JPG.rf.e93810be910dc220a82df9f57b84d1e6.jpg",
        "DSC05529_JPG.rf.4f3713360f6ffd96b6f246b0a98f7ecc.jpg",
        "DSC05529_JPG.rf.62abad02ff6e1e6496779e29f41414e7.jpg",
        "DSC05529_JPG.rf.b2794e72e9c127da672c08c141e0f493.jpg",
        "DSC05532_JPG.rf.939ade46604c88a80790f0602780d083.jpg",
        "DSC05532_JPG.rf.e2c1102d501f88ecc85c72c17db12d7e.jpg",
        "DSC05532_JPG.rf.f57277875049591287667b44c6c77018.jpg",
        "DSC05533_JPG.rf.1f1843c77dcf455bf7c36df857f4fe6d.jpg",
        "DSC05533_JPG.rf.2cedaa5fff0ffa47bba8d2d3ccceebb8.jpg",
        "DSC05533_JPG.rf.5588a1d44548dda85e44f561128ce0ff.jpg",
        "DSC05536_JPG.rf.182cfdc5e8f70d3b8c6e1c73c4fd5540.jpg",
        "DSC05536_JPG.rf.33276414c793dbf3f438476240d3e7d9.jpg",
        "DSC05536_JPG.rf.936bc4fb50341772d15238a9af532632.jpg",
        "DSC05537_JPG.rf.1c19f7e2ad800812357240bf8a48f0a6.jpg",
        "DSC05537_JPG.rf.540553851006eae3f0001ca644aa3e8c.jpg",
        "DSC05537_JPG.rf.bb82406713816127048502a6ab2fcb33.jpg",
        "DSC05539_JPG.rf.2fdfbc009677d88ebdbc7fff5f3b1ad9.jpg",
        "DSC05539_JPG.rf.5e68159a6a81bbdd79e81774f2302c00.jpg",
        "DSC05539_JPG.rf.863b266570b6cd231b747781c5d86a53.jpg",
        "DSC05540_JPG.rf.2c0ef7e242b6eaae9163b1e8b1f16d73.jpg",
        "DSC05540_JPG.rf.51b102300801d80d61444ce4a898638f.jpg",
        "DSC05540_JPG.rf.e46fc19afa04514588eeda7a4fe704a0.jpg",
        "DSC05542_JPG.rf.5a7adca4f1fcc97534265644caf4a11f.jpg",
        "DSC05542_JPG.rf.663dccdb7aa40a8816dc4e320354d836.jpg",
        "DSC05542_JPG.rf.9fe892b51f7c026e2102a74841d80929.jpg",
        "DSC05543_JPG.rf.394d6d56005bdbb5ce90c415f977810d.jpg",
        "DSC05543_JPG.rf.b6c1ea974c2d7ffce637ca66f37aa543.jpg",
        "DSC05543_JPG.rf.ed01c56cb5c24894b21c4692117de954.jpg",
        "DSC05544_JPG.rf.492a5e7404c4ae99384aa878fdf5be1f.jpg",
        "DSC05544_JPG.rf.94b92f1200ea6e1ea187741a247731b1.jpg",
        "DSC05544_JPG.rf.b930e2a4e28d0e84ad6e228cd82cf8c9.jpg",
        "DSC05548_JPG.rf.8f8fdc3da995fa98bef035acf6881c09.jpg",
        "DSC05548_JPG.rf.b7814ad616e20f6a6112bce6f0af895b.jpg",
        "DSC05548_JPG.rf.ebb726484cc1c8192fdc5563d09ae830.jpg",
        "DSC05551_JPG.rf.19319df32191876bba7b3faf22058682.jpg",
        "DSC05551_JPG.rf.9932cff296f70505017079bc2ef1d3e1.jpg",
        "DSC05551_JPG.rf.d7848e9e98c0facfffd1389c7dccf81a.jpg",
        "DSC05552_JPG.rf.4e8b5ae5ad5d7cb2736da150bcbce15a.jpg",
        "DSC05552_JPG.rf.b97281d9c04bc9a13e84777ed8314c24.jpg",
        "DSC05552_JPG.rf.fe5daab9832f5752b2190a050b09884f.jpg",
        "DSC05553_JPG.rf.3bf439effbb0d3c297a05bc2b19058c8.jpg",
        "DSC05553_JPG.rf.92c9cb83ed8e22e3e2a319e79ee64c50.jpg",
        "DSC05553_JPG.rf.af15b8b0bb9c9c21066974fd3dca38dc.jpg",
        "DSC05554_JPG.rf.1f1d92a5c47bc77b0e5fd46345e4e19a.jpg",
        "DSC05554_JPG.rf.21e32dd08bd4122d6b45c38a4ae86b87.jpg",
        "DSC05554_JPG.rf.b107da3b0c4931ab2b819b2363a8dbd9.jpg",
        "DSC05557_JPG.rf.6c3eb362962cd40d989e345400b08c5a.jpg",
        "DSC05557_JPG.rf.c39112f002775d3485a25c5c3dddc6cb.jpg",
        "DSC05557_JPG.rf.d5d6e841cedb52118b792ca72e0525f0.jpg",
        "DSC05559_JPG.rf.142874a226c512d39021bcbb06610b21.jpg",
        "DSC05559_JPG.rf.56bff2ed3fc1dd620eb33a77b8d5759b.jpg",
        "DSC05559_JPG.rf.e5677d37a10f209b399d0fa35a1683b4.jpg",
        "DSC05561_JPG.rf.417f20574ed9f2e14eed29cf91b18afa.jpg",
        "DSC05561_JPG.rf.5109885231864f783fbdd0f1e41cdf7d.jpg",
        "DSC05561_JPG.rf.6b77b784ff24e30903b89ed2c9e234cb.jpg",
        "DSC05563_JPG.rf.c80df39edeea6014497d8166f43cfdae.jpg",
        "DSC05563_JPG.rf.d01d7055b140d9b1471866b836c555f3.jpg",
        "DSC05563_JPG.rf.d175d6abe2e74ff4d4ac6bb79d58f76d.jpg",
        "DSC05564_JPG.rf.03acaa0f5626eb3eaab28b315aefdf00.jpg",
        "DSC05564_JPG.rf.808548e58301c6a67993444656c92df7.jpg",
        "DSC05564_JPG.rf.d67d02ce40857c603d7dfccae3193a89.jpg",
        "DSC05565_JPG.rf.0f83ede81256a0b1949616086557c117.jpg",
        "DSC05565_JPG.rf.27420a8bc7ab03ade022bc77acefb735.jpg",
        "DSC05565_JPG.rf.5cd9da45033aa18551f2cff875107ea3.jpg",
        "DSC05566_JPG.rf.0cf5b9847facee2e4aa89f13dd8365a2.jpg",
        "DSC05566_JPG.rf.21f3b218583c8c5613b05a9b93d3d23a.jpg",
        "DSC05566_JPG.rf.2d80aa7b0f78fd2729dba97d2ccdeb9f.jpg",
        "DSC05568_JPG.rf.1c53e328459ff33836d995a95d845456.jpg",
        "DSC05568_JPG.rf.6260c70c753d33d184a4cab5f552f194.jpg",
        "DSC05568_JPG.rf.ca90ce82b91465905325840846199923.jpg",
        "DSC05569_JPG.rf.8288bf4781aff9f28607e4eda6f47ead.jpg",
        "DSC05569_JPG.rf.872cb8ea211831fa8678a8d4ae6d7a15.jpg",
        "DSC05569_JPG.rf.e51e1ba3fab2c9e4ab46ff85d0516d98.jpg",
        "DSC05570_JPG.rf.3256580a5c9bbbb2428b551686f6ff99.jpg",
        "DSC05570_JPG.rf.75a4e78de1998f9466eae3e8a7baee57.jpg",
        "DSC05570_JPG.rf.dc777919732213a4332868ea177ef0fa.jpg",
        "DSC05571_JPG.rf.6d773808ecb110b2fa93d586f7024657.jpg",
        "DSC05571_JPG.rf.6de72ccfe146e4a2ceceb7987d5253c2.jpg",
        "DSC05571_JPG.rf.cee56da64e720622dcb06f8d21889f6f.jpg",
        "DSC05574_JPG.rf.4843581250199a0db5cd3f6d6b45798b.jpg",
        "DSC05574_JPG.rf.56bb49a2bc9f73b60f432545d7829b99.jpg",
        "DSC05574_JPG.rf.8e912da9a3d943c6733e4dd3e2f65d7f.jpg",
        "gateway_mp4-0001_jpg.rf.1ff85b1309b22872eada5bb76c6f44fe.jpg",
        "gateway_mp4-0001_jpg.rf.7253ad6d723ca62a8d2496756177b62a.jpg",
        "gateway_mp4-0001_jpg.rf.97cd53e6044af3b3c37079e6c41e7387.jpg",
        "gateway_mp4-0003_jpg.rf.29ecdb161836e533cb0c393f203be0c6.jpg",
        "gateway_mp4-0003_jpg.rf.71ac5e9ebe97dd134267c3e2e48925bc.jpg",
        "gateway_mp4-0003_jpg.rf.a0843b2b16cc5c45aaf6e778e1039c3b.jpg",
        "gateway_mp4-0014_jpg.rf.07ec1ae480e01f06fff05057a140465d.jpg",
        "gateway_mp4-0014_jpg.rf.081c7cddad25b5ea2a43ca8a98547f17.jpg",
        "gateway_mp4-0014_jpg.rf.993123fd7290d7dd03be232ad7c2556c.jpg",
        "gateway_mp4-0018_jpg.rf.47c9b8a1027a9530b62052bbe7c867c2.jpg",
        "gateway_mp4-0018_jpg.rf.601ffdd1bb0a32448e5e0496128d6ded.jpg",
        "gateway_mp4-0018_jpg.rf.e308a5223f03b40340d814e2715ec243.jpg",
        "gateway_mp4-0027_jpg.rf.2978313cd5b7400fb09f4b897ae21831.jpg",
        "gateway_mp4-0027_jpg.rf.3ce3c6c2c1955798055fdd16c7e3fc2b.jpg",
        "gateway_mp4-0027_jpg.rf.cb4e42e8cd8e973f14f7b863ffb197cc.jpg",
        "gateway_mp4-0032_jpg.rf.435db50e5fc95fa06a8168b7c374e2ab.jpg",
        "gateway_mp4-0032_jpg.rf.4568b31b6ff57a0fc86686cc737e87a9.jpg",
        "gateway_mp4-0032_jpg.rf.7401f52219784e039f0c4d0187803001.jpg",
        "gateway_mp4-0039_jpg.rf.07da19c890088a3e78b600dc31d49cc3.jpg",
        "gateway_mp4-0039_jpg.rf.12def02dd8a2d5df48da88e4d829d158.jpg",
        "gateway_mp4-0039_jpg.rf.556ad8e6e73c357f773bb0827540cb13.jpg",
        "IMG_5530_jpg.rf.6198aa97024b1af701731926b14f62d7.jpg",
        "IMG_5530_jpg.rf.82e3c295a466eb67ab49aa2f52c2b6a8.jpg",
        "IMG_5530_jpg.rf.d21be477eb78e988457a06d72d8a3501.jpg",
        "IMG_5531_jpg.rf.091ab34ae98573b73878321951336112.jpg",
        "IMG_5531_jpg.rf.3cffd9a17922239a1896b8ba8130d868.jpg",
        "IMG_5531_jpg.rf.d4c446b847a2439707c424caf3ea3df7.jpg",
        "IMG_5532_jpg.rf.1481c9b54d74f781dc32e59b6c18e4be.jpg",
        "IMG_5532_jpg.rf.53b8dd2112b6414683e6e43f12cf87e0.jpg",
        "IMG_5532_jpg.rf.615e7d94a832846185c53e88474b2dbb.jpg",
        "IMG_5534_jpg.rf.590b892cdeb4b95be6b0f9f3ebd22cf2.jpg",
        "IMG_5534_jpg.rf.5fe3384300f422c3f8602f3c8c4d83ab.jpg",
        "IMG_5534_jpg.rf.a9383cfa01741511a40df0a2cbdfaa8d.jpg",
        "IMG_5536_jpg.rf.97e19306a54fc4e44b18a2ed3a9a1ef6.jpg",
        "IMG_5536_jpg.rf.b588138e02b68f5d5c3bbf81b4b84ba1.jpg",
        "IMG_5536_jpg.rf.ee795090d20980a57bd3a5ddde1fb0a0.jpg",
        "video_mp4-0000_jpg.rf.37196aca18ace4bad7a62a3007a79931.jpg",
        "video_mp4-0000_jpg.rf.7311cc4acbd89472b3ed5460da8830b2.jpg",
        "video_mp4-0000_jpg.rf.d16d31dee63e4d272bdfd9eb163ef265.jpg",
        "video_mp4-0002_jpg.rf.0fee5135b33bed83fac66e60ed41294c.jpg",
        "video_mp4-0002_jpg.rf.1a7ea5d180392fc0cdd0fb53f05dd9be.jpg",
        "video_mp4-0002_jpg.rf.8404d433b3bfcfb0cd866f79c1d9e4bb.jpg",
        "video_mp4-0003_jpg.rf.715f7073c2d49a72758d0fad17671a67.jpg",
        "video_mp4-0003_jpg.rf.9bc91ce1f2b3f7a03cf50b7ef3d34f50.jpg",
        "video_mp4-0003_jpg.rf.e98d755215a2a402db6d762bd852d071.jpg",
        "video_mp4-0004_jpg.rf.68fc131802c5102d777c7cf526665a2a.jpg",
        "video_mp4-0004_jpg.rf.8271ab945740e056803563b0ab97c923.jpg",
        "video_mp4-0004_jpg.rf.9dbb6e9b397c37560b27c22fc2b26eeb.jpg",
        "video_mp4-0005_jpg.rf.29838c8137c2c82a84fb9a3f9967ab4f.jpg",
        "video_mp4-0005_jpg.rf.c2596184d1eb191f78d2c1c71bb097bd.jpg",
        "video_mp4-0005_jpg.rf.c8e0f5f7756f0a85fc634b9033db2fff.jpg",
        "video_mp4-0006_jpg.rf.0eb554421a459c249cdcfee51a0b5afd.jpg",
        "video_mp4-0006_jpg.rf.504ff4a1e71d80d498500a401384a15e.jpg",
        "video_mp4-0006_jpg.rf.c3f63d9782db3f16fcd0727484b48a83.jpg",
        "video_mp4-0007_jpg.rf.41f48d9377a4616c4b5e74f62735f2ee.jpg",
        "video_mp4-0007_jpg.rf.4a1a9943ad3b6d66f7f28018b6e635b6.jpg",
        "video_mp4-0007_jpg.rf.dabeb413af944ebc01f53bafd91f6c7c.jpg",
        "video_mp4-0008_jpg.rf.b740ceb6e68c50f878a2a0479470e0f0.jpg",
        "video_mp4-0008_jpg.rf.c5e22cf63b522993ceb75654e1b52908.jpg",
        "video_mp4-0008_jpg.rf.fea6c8967614dce181be63f3eaa1a53e.jpg",
        "video_mp4-0009_jpg.rf.5fb3b55e36df03352418433dac3c4264.jpg",
        "video_mp4-0009_jpg.rf.af68ae4b499bf6785572b94568b71ad3.jpg",
        "video_mp4-0009_jpg.rf.de1e7c6faeaf95d5e880a2c14165ca50.jpg",
        "video_mp4-0010_jpg.rf.0aab80deae9b16185c22272ecf7ea936.jpg",
        "video_mp4-0010_jpg.rf.2dafed33a4b8a7402eb443bc78cf7816.jpg",
        "video_mp4-0010_jpg.rf.e05834fa1503f7f4eade7f00aaa4a549.jpg",
        "video_mp4-0011_jpg.rf.5cf541200c2f59db9c20896648d1459e.jpg",
        "video_mp4-0011_jpg.rf.7ee3f027bf86bdc75f1dded2732d3de6.jpg",
        "video_mp4-0011_jpg.rf.91920068f5cd015c4daf9a0dcec7a8b8.jpg",
        "video_mp4-0013_jpg.rf.3ec2cfd1e833607398e8f9414f013792.jpg",
        "video_mp4-0013_jpg.rf.98cb7d823582a7c907a8389d0afb61e4.jpg",
        "video_mp4-0013_jpg.rf.cb40445f00e30f93ce5ee2d8047c2a8a.jpg",
        "video_mp4-0015_jpg.rf.6c82b7ef9180641ccf5f38289e3f975c.jpg",
        "video_mp4-0015_jpg.rf.9d3187d39e0da905cbf0b00be31c6a2a.jpg",
        "video_mp4-0015_jpg.rf.c77e279e99365dd354bee38b5fca6b9d.jpg",
        "video_mp4-0016_jpg.rf.41298f3384097e536d7871d67cf55b39.jpg",
        "video_mp4-0016_jpg.rf.8e3aa385a6bb8e5fd586057b29abfae7.jpg",
        "video_mp4-0016_jpg.rf.eebaf6b05a25af02b75d17cbb3a29f7c.jpg",
        "video_mp4-0018_jpg.rf.89824d9f3da3a0db4cf6b55e3d805651.jpg",
        "video_mp4-0018_jpg.rf.95f4d40b54675d8f195af706134e6fdd.jpg",
        "video_mp4-0018_jpg.rf.ffea5772dacab095fceb394ef9de4583.jpg",
        "video_mp4-0020_jpg.rf.4d2435dda995dd93432674c5011b3ef3.jpg",
        "video_mp4-0020_jpg.rf.edfbcc1fccb636fb326ae001dc75b726.jpg",
        "video_mp4-0020_jpg.rf.eed6cd24b9fdc5ba06ded2f56429e247.jpg",
        "video_mp4-0021_jpg.rf.4e0b21a32fd2d6a63ba9c7677aa93765.jpg",
        "video_mp4-0021_jpg.rf.563dfffa491bbb6cd0db6ab98d62392f.jpg",
        "video_mp4-0021_jpg.rf.fedf30ef0a60a206d8c608749f6c9444.jpg",
        "video_mp4-0023_jpg.rf.ad755c0915a07622c5fe8ca80bac6267.jpg",
        "video_mp4-0023_jpg.rf.d3d402473b388857d09a34278e323139.jpg",
        "video_mp4-0023_jpg.rf.ec21073c38ed7450aa4599a6dcef7cf3.jpg",
        "video_mp4-0025_jpg.rf.75052bf7184f9ded926e0fccf62b321f.jpg",
        "video_mp4-0025_jpg.rf.830def5f6499d793c4d2b9bfffcecb03.jpg",
        "video_mp4-0025_jpg.rf.ad73e2c7b39f464b396d19d696e0c8a6.jpg",
        "video_mp4-0026_jpg.rf.596e03365fe81a5a3e7b31b8c6978c14.jpg",
        "video_mp4-0026_jpg.rf.c1235ac254f3e93a7b11d6960be9520c.jpg",
        "video_mp4-0026_jpg.rf.e9ea6dc4b10131cda18cf5514be971b0.jpg",
        "video_mp4-0028_jpg.rf.4f0cbb01a701bb50db44c54cffeb6a61.jpg",
        "video_mp4-0028_jpg.rf.a84f8ff964d7ebd9df5aedb4537d23b0.jpg",
        "video_mp4-0028_jpg.rf.f12a0b854326a566fd97c6b151cdf057.jpg",
        "video_mp4-0029_jpg.rf.0622ad4d633178391c6b26de95f1563c.jpg",
        "video_mp4-0029_jpg.rf.4090f2e0041249650b731718771b320d.jpg",
        "video_mp4-0029_jpg.rf.4c60777c02c2fd2493af59e7923b3fb5.jpg",
        "video_mp4-0030_jpg.rf.1290cd86efea44e176b85d4f56f45ce9.jpg",
        "video_mp4-0030_jpg.rf.e96f9eb9d142c8a5581de89cb430fa82.jpg",
        "video_mp4-0030_jpg.rf.ece2dd2089768e69b57a4bb14432fb18.jpg",
        "video_mp4-0031_jpg.rf.05a7be56a4e5a1ed18e8dfdcd13dcca1.jpg",
        "video_mp4-0031_jpg.rf.d890e93de1247a578e3387ea9fe4f0fd.jpg",
        "video_mp4-0031_jpg.rf.fc9993484a03d0989d326dd14ade5305.jpg",
        "video_mp4-0036_jpg.rf.208f883edbc44585a6cbe3448f01ca10.jpg",
        "video_mp4-0036_jpg.rf.281b493bb9f77a4f2ac0feb56d08592d.jpg",
        "video_mp4-0036_jpg.rf.aa54f542d2acb6ded270c77e74a7aaaf.jpg",
        "video_mp4-0038_jpg.rf.9d78bef3355f6b925ba5941f21e09cc0.jpg",
        "video_mp4-0038_jpg.rf.ba88930f85b938a1c500b9c1cedb35bc.jpg",
        "video_mp4-0038_jpg.rf.cb99a794ecb1cd19a757272fb7e7e8ca.jpg",
        "video_mp4-0039_jpg.rf.260b06ab2c2e57ae3038d33bd0d2d978.jpg",
        "video_mp4-0039_jpg.rf.2e3a42663b83336e30b21fa7a250a8a7.jpg",
        "video_mp4-0039_jpg.rf.96794ce5d7ad58f643b9fa94dd6b4dd1.jpg",
        "video_mp4-0042_jpg.rf.2e40670e35c9ddc2b3a27aa661f72637.jpg",
        "video_mp4-0042_jpg.rf.906531b9b301ea98f948eb4070d67f02.jpg",
        "video_mp4-0042_jpg.rf.f7ab378edec890ae14f8ec55f158f23a.jpg",
        "video_mp4-0043_jpg.rf.d2be9bc0b8d45ad9a7a45b6aba478883.jpg",
        "video_mp4-0043_jpg.rf.db8a94094396a1ec08f54dbf899c0290.jpg",
        "video_mp4-0043_jpg.rf.f0ef9d6acf687245b4a596d194fc6344.jpg",
    ]

    start = time.time()
    for i in range(len(IMAGES)):
        print(f"Processing image {i+1}/{len(IMAGES)}")
        process_image(f'/home/aadish/AIWorlds/eval/eval_images/{IMAGES[i]}')
    start = time.time() - start
    output_text = f"\nEval on {datetime.datetime.now()}\n"
    output_text += f"Total time taken: {start:.2f} seconds for {len(IMAGES)} images\n"
    output_text += f"Average time per image: {start/len(IMAGES):.2f} seconds\n"
    output_text += f"Images per second: {len(IMAGES)/start:.2f}\n"

    print(output_text, end="")
    path = './eval_output.txt'
    os.system('echo "' + output_text.replace('\n',
              '\\n') + f'" >> {path}')

if __name__ == "__main__":
    with InferenceEngine("/home/aadish/AIWorlds/comp2/yolov5n-best.engine") as engine:
        def process_image(i):
            engine.run(cv2.imread(i))
        run_eval(process_image)
```

```txt AIWorlds/comp2/eval_output.txt
Eval on 2025-04-23 21:10:09.782904 => v5s test
Total time taken: 22.80 seconds for 207 images
Average time per image: 0.11 seconds
Images per second: 9.08 => 8.47 (old code w/o NMS)

Eval on 2025-04-23 21:11:06.674479 => v5n test
Total time taken: 14.26 seconds for 207 images
Average time per image: 0.07 seconds
Images per second: 14.52
```

```py AIWorlds/comp2/inference.py
import cv2
import numpy as np
import pycuda.driver as cuda
import tensorrt as trt
import torch
import torchvision as vision


class InferenceEngine:
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

    def __init__(
        self,
        engine_path: str,
        nms_params={
            "conf_thres": 0.20,
            "iou_thres": 0.5,
            "max_det": 50,
            "classes": ["blue", "goal", "red", "bot"],
        },
        input_shape=(1, 3, 640, 640),
        output_shape=(1, 25200, 9),
        device_id=0,
    ):
        # data
        self.engine_path = engine_path
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.nms_params = nms_params
        # set up CUDA device
        cuda.init()
        self.device = cuda.Device(device_id)
        # make_context() pushes the new context on the stack
        self.cuda_ctx = self.device.make_context()

        with open(self.engine_path, "rb") as f, trt.Runtime(self.TRT_LOGGER) as runtime:
            self.engine = runtime.deserialize_cuda_engine(f.read())
        if self.engine is None:
            raise SystemExit(f"Failed to load engine '{engine_path}'")
        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers(
            self.engine
        )
        self.trt_context = self.engine.create_execution_context()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    # MARK: - Inference utilities
    def _allocate_buffers(self, engine):
        inputs, outputs, bindings = [], [], []
        stream = cuda.Stream()
        for binding in engine:
            size = trt.volume(engine.get_binding_shape(
                binding)) * engine.max_batch_size
            dtype = trt.nptype(engine.get_binding_dtype(binding))

            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            bindings.append(int(device_mem))

            if engine.binding_is_input(binding):
                inputs.append({"host": host_mem, "device": device_mem})
            else:
                outputs.append({"host": host_mem, "device": device_mem})

        return inputs, outputs, bindings, stream

    def _do_preprocessing(self, img):
        img = cv2.resize(img, (self.input_shape[3], self.input_shape[2]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.astype(np.float32) / 255.0
        img = img.transpose((2, 0, 1))[None, ...]  # NHWC->NCHW
        img = np.ascontiguousarray(img)

        # Copy to host buffer
        np.copyto(self.inputs[0]["host"], img.ravel())

    def _do_inference(self):
        for inp in self.inputs:
            cuda.memcpy_htod_async(inp["device"], inp["host"], self.stream)
        self.trt_context.execute_async_v2(
            self.bindings, stream_handle=self.stream.handle
        )
        for out in self.outputs:
            cuda.memcpy_dtoh_async(out["host"], out["device"], self.stream)
        self.stream.synchronize()
        return [out["host"] for out in self.outputs]

    def close(self):
        # 1) sync & free your PyCUDA streams/bindings
        self.stream.synchronize()
        self.stream = None
        for entry in (self.inputs or []) + (self.outputs or []):
            entry["device"].free()
        self.inputs = None
        self.outputs = None

        # 2) explicitly drop TRT objects while context is still current
        #    this will invoke their __del__ (and hence free any TRT internal
        #    allocations) under the proper context
        del self.trt_context
        del self.engine
        self.trt_context = None
        self.engine = None

        # 3) now it’s safe to pop the context
        self.cuda_ctx.pop()
        self.cuda_ctx = None

    # MARK: - NMS utilies
    def _xywh2xyxy(self, boxes):
        # boxes: Tensor[M,4] = [xc,yc,w,h]
        x_c, y_c, w, h = boxes.unbind(1)
        return torch.stack((x_c - w / 2, y_c - h / 2, x_c + w / 2, y_c + h / 2), dim=1)

    @torch.no_grad()
    def _cpu_nms(
        self,
        pred: torch.Tensor,
        conf_thres: float = None,
        iou_thres: float = None,
        max_det: int = None,
    ) -> torch.Tensor:
        if conf_thres is None:
            conf_thres = self.nms_params["conf_thres"]
        if iou_thres is None:
            iou_thres = self.nms_params["iou_thres"]
        if max_det is None:
            max_det = self.nms_params["max_det"]
        """
        pred: Tensor[1, N, 5+nc] = [xc, yc, w, h, obj_conf, cls_conf_0, ..., cls_conf_nc-1]
        returns: Tensor[K,6] = [x1,y1,x2,y2,score,cls_idx]
        """
        x = pred.squeeze(0)  # [N,5+nc]
        if x.numel() == 0:
            return x.new_zeros((0, 6))

        # 1) objectness × best class confidence
        obj_conf = x[:, 4]  # [N]
        cls_conf_vals, cls_idx = x[:, 5:].max(dim=1)  # both [N]
        scores = obj_conf * cls_conf_vals  # [N]

        # 2) threshold
        mask = scores > conf_thres
        if not mask.any():
            return x.new_zeros((0, 6))

        x = x[mask]
        scores = scores[mask]
        cls_idx = cls_idx[mask]

        # 3) to corner format
        boxes = self._xywh2xyxy(x[:, :4])  # [M,4]

        keep = vision.ops.nms(boxes, scores, iou_thres)
        if keep.numel() > max_det:
            keep = keep[:max_det]

        # 5) gather detections
        det = torch.cat(
            (
                boxes[keep],
                scores[keep].unsqueeze(1),
                cls_idx[keep].unsqueeze(1).float(),
            ),
            dim=1,
        )  # [K,6]

        return det

    def _do_nms(self, out3d):
        det = self._cpu_nms(torch.from_numpy(out3d))
        if det.numel() == 0:
            return None
        det = det.numpy()  # [[x1,y1,x2,y2,score,cls], ...]

        detections = []
        for x1, y1, x2, y2, conf, cls in det:
            d = {
                "x": float((x1 + x2) / 2),
                "y": float((y1 + y2) / 2),
                "width": float(x2 - x1),
                "height": float(y2 - y1),
                "class": self.nms_params["classes"][int(cls)],
                "depth": float(0),
                "confidence": float(conf),
            }
            detections.append(d)
        return detections

    # MARK: - main entry point
    def run(self, img):
        # every frame:
        # preprocessing -> input -> tensorrt -> output -> postprocessing (nms) -> json blob
        self._do_preprocessing(img)
        trt_outs = self._do_inference()

        raw = trt_outs[0]
        try:
            out3d = raw.reshape(self.output_shape)
        except ValueError as e:
            print(
                f"ERROR: cannot reshape {raw.size} → {self.output_shape}: {e}")
            return []
        return self._do_nms(out3d) or []
```

```py AIWorlds/comp2/localization.py


class Localization:
    def __init__(self, initial_pose):
        self.pose = initial_pose # (x, y, θ)
    def update(self, measurement, imu):
        # MARK: - THIS PART OF THE CODE WRITTEN BY OLIVER

        # imu is the imu heading in centidegrees (from 0 to 360*100)
        # measurement is a numpy array of shape (640,) representing a row of depth measurements in meters
        # use imu and measurement to update self.pose
        # this function needs to run in less than 1/30 of a second
        pass
```

```py AIWorlds/comp2/main.py
import threading
import time
import signal
import logging
import numpy as np
import copy
from flask import Flask, stream_with_context, Response
from flask_cors import CORS
import cv2
import json

from camera import Camera, Processing
from inference import InferenceEngine
import comms
import localization

"""
what they send us: {x?, y?, theta}
what we send them: {
stuff: [{x, y, width, height, class(str), depth, confidence}]
flag(str)
pose {x, y, theta}
}
"""


class Worker:
    def __init__(self, engine_path, row):
        # camera stuff
        self.camera = Camera()
        self.camera.start()
        self.processing = Processing(self.camera.depth_scale)
        self.current_detections_raw = []
        self.current_detections = []
        self.current_frames = (
            np.zeros((480, 640, 3), dtype=np.uint8), np.zeros((480, 640), dtype=np.uint8), np.zeros((480, 640), dtype=np.uint8))
        # inference stuff
        self.engine = InferenceEngine(engine_path)
        # serial stuff
        self.measurement_row = row
        self.localization = None

    def _serial_callback(self, data):
        print("serial callback")
        def keep_ascii(s): return "".join(
            c for c in s if ord(c) < 128 and ord(c) > 0)
        # HANDLE FLAGS
        flag = ""
        # HANDLE LOCALIZATION
        data = keep_ascii(data)
        try:
            data = json.loads(data)
            # print(data, "worked")
        except json.JSONDecodeError:
            # print(
            #     ' '.join(list(map(lambda a: f'|{a}|{ord(a)}|', data.lstrip()))))
            # print(f"Error decoding JSON: {e}")
            return None
        if self.localization is None:
            x = data.get('x', 0)
            y = data.get('y', 0)
            theta = data.get('theta', 0)
            self.localization = localization.Localization((x, y, theta))
            pose = data
        else: # self.localization is not None:
            theta = data.get('theta', 0)
            measurement = self.current_frames[2][self.measurement_row]
            self.localization.update(measurement, theta)
            pose = {
                'x': self.localization.pose[0],
                'y': self.localization.pose[1],
                'theta': self.localization.pose[2]
            }

            # update flag given measurement
            percentile = np.percentile(measurement[measurement > 0], 20) * self.processing.depth_scale
            print(percentile * 3.28)
            if percentile * 3.28 < 1: # that is, at least 20% of nonzero readings are less than 1 foot away
                flag = "STOP"
        # HANDLE OBJECTS
        objects = self.current_detections
        return json.dumps({
            'pose': pose,
            'stuff': objects,
            'flag': flag
        }, indent=None, separators=(',', ':'))

    def serial_worker(self):
        print("serial worker")

        self.localization = None
        self.serial = comms.EventDrivenSerial(
            "/dev/ttyACM1", 115200, self._serial_callback)  # comms.prompt_user_for_port(comms.list_serial_ports())
        self.serial.serial_worker()

    def camera_worker(self):
        print("camera worker")
        try:
            while True:
                # camera stuff
                frames = self.camera.get_frames()
                depth_image, color_image, depth_map = self.processing.process_frames(
                    frames)
                self.current_frames = (color_image, depth_map, depth_image)
                # processing stuff
                detections = copy.deepcopy(self.current_detections_raw)
                for detection in detections:
                    # 480, 640
                    detection['y'] *= 480 / 640
                    depth_value = self.processing.get_depth(
                        detection, depth_image)
                    if np.isnan(depth_value):
                        detection['depth'] = -1
                    else:
                        detection['depth'] = depth_value
                self.current_detections = [
                    i for i in detections if
                    i['class'] not in ['red', 'blue'] or
                    i['confidence'] > 0.6
                ]
                try:
                    self._serial_callback('{"imu": 150.0}')
                except Exception as e:
                    print(f"Error sending serial data: {e}")
        finally:
            self.camera.stop()

    def inference_worker(self):
        print("inference worker")
        self.engine.cuda_ctx.push()
        MIN_LATENCY = 1.0 / 20.0
        try:
            while True:
                img = self.current_frames[0]
                start_time = time.time()
                self.current_detections_raw = self.engine.run(img)
                time_elapsed = time.time() - start_time
                if time_elapsed < MIN_LATENCY: # cap at 20 fps
                    time.sleep(MIN_LATENCY - time_elapsed)

        finally:
            # pop when you exit, so you don’t leak
            self.engine.cuda_ctx.pop()
            self.engine.close()

    def app_worker(self):
        # stop excessive logging of requests
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.ERROR)

        def generate(type):
            boundary = b'--frame'
            while True:
                # Grab the latest frame + detections
                color_img, depth_map = self.current_frames
                img = (color_img if type == 'color' else depth_map if type ==
                       'depth' else np.zeros_like(color_img)).copy()

                # Draw detections onto img
                for d in self.current_detections:
                    x, y = d['x'], d['y']
                    w, h = d['width'], d['height']
                    cls = d['class']
                    conf = d['confidence']
                    depth = d.get('depth', None)

                    # top‑left corner
                    x0 = int(x - w/2)
                    y0 = int(y - h/2)
                    x1 = int(x + w/2)
                    y1 = int(y + h/2)

                    # choose a color
                    color = (0, 0, 255)
                    if cls == 'blue':
                        color = (255, 0, 0)
                    elif cls == 'goal':
                        color = (0, 215, 255)
                    elif cls == 'red':
                        color = (0, 0, 255)
                    elif cls == 'bot':
                        color = (0, 0, 0)

                    # rectangle
                    cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)

                    # label text
                    label = f"{cls} {conf:.2f}"
                    if depth is not None and depth >= 0:
                        label += f" d={depth:.2f}m"
                    # putText above box
                    t_w, t_h = cv2.getTextSize(
                        label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    txt_y = y0 - 5 if y0 - 5 > 10 else y0 + t_h + 5
                    cv2.rectangle(img, (x0, txt_y - t_h - 4),
                                  (x0 + t_w + 4, txt_y + 2), color, -1)
                    cv2.putText(
                        img, label, (x0+2, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                # JPEG‐encode
                success, jpg = cv2.imencode(
                    '.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
                if not success:
                    continue
                frame = jpg.tobytes()

                # yield multipart chunk
                yield boundary + b'\r\n' \
                    b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n'

                # throttle to ~30fps
                time.sleep(1/30)



        app = Flask("3151App")
        CORS(app, resources={r"/*": {"origins": "*"}})

        @app.route('/<type>.mjpg')
        def mjpeg_stream(type):
            """
            MJPEG stream of color frames with overlaid detections.
            """
            return Response(
                stream_with_context(generate(type)),
                mimetype='multipart/x-mixed-replace; boundary=frame'
            )

        @app.route('/events')
        def sse_events():
            """
            Server‑Sent Events stream of raw detection JSON.
            """
            def generate():
                while True:
                    data = {"stuff": self.current_detections}
                    payload = f"data: {json.dumps(data)}\n\n"
                    yield payload
                    time.sleep(1/30)

            headers = {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
            return Response(stream_with_context(generate()), headers=headers)

        # A simple landing page to consume them
        @app.route('/')
        def index():
            with open('visualization.html', 'r') as f:
                return f.read()
        # Run Flask
        app.run(host='0.0.0.0', port=5000, threaded=True, debug=False)


"""
List of workers:
    Camera worker
        * Flagship worker, always running at 30 fps
        * Gets most recent camera frames
    Inference worker
        * Runs below 10 fps
        * Blocking computations on GPU for YOLOv5s/n model
    App worker
        * Generally idle
        * Serves Flask dashboard at localhost:5000 or [ip]:5000
    Serial worker
        * Reads serial data from USB port
        * Responds in JSON
"""
if __name__ = "main":
    shutdown = threading.Event()

    def handle_sigterm(signum, frame):
        shutdown.set()


    signal.signal(signal.SIGINT, handle_sigterm)
    signal.signal(signal.SIGTERM, handle_sigterm)
    # initialize worker
    worker = Worker("/home/aadish/AIWorlds/comp2/yolov5n-best.engine", 240)

    threads = []
    # camera worker
    t1 = threading.Thread(target=worker.camera_worker, daemon=True)
    threads.append(t1)
    # inference worker
    t2 = threading.Thread(target=worker.inference_worker, daemon=True)
    threads.append(t2)
    # app worker
    t3 = threading.Thread(target=worker.app_worker, daemon=True)
    threads.append(t3)
    # serial worker
    t4 = threading.Thread(target=worker.serial_worker, daemon=True)
    threads.append(t4)

    # start them all
    for t in threads:
        t.start()

    # now block here until CTRL‑C or SIGTERM
    shutdown.wait()

    # clean up
    worker.model.close()
    # if your threads check shutdown flag, they can exit cleanly
    for t in threads:
        t.join(timeout=1)
```

```html AIWorlds/comp2/visualization.html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>3151A Web Dashboard</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    html, body {
      margin: 0;
      padding: 0;
      background: #f8fafc;
      min-height: 100vh;
    }
    body {
      font-family: Arial, sans-serif;
      text-align: center;
    }
    #hi {
      height: 5vh;
      object-fit: cover;
      object-position: center;
      margin-bottom: 0.5rem;
      border-bottom: 2px solid #dee2e6;
    }
    .dashboard-title {
      font-weight: 700;
      letter-spacing: 1px;
      margin-bottom: 1.5rem;
      color: #212529;
      text-shadow: 0 1px 0 #fff, 0 2px 4px rgba(0,0,0,0.04);
    }
    .collapsible-header {
      display: flex;
      align-items: center;
      justify-content: flex-start;
      cursor: pointer;
      padding: 0.75rem 1.25rem;
      background-color: #f1f3f5;
      margin: 0 auto;
      max-width: 700px;
      font-size: 1.15rem;
      font-weight: 500;
      border: 1px solid #dee2e6;
      border-bottom: none;
      transition: background 0.15s;
      box-shadow: 0 1px 4px rgba(0,0,0,0.03);
      user-select: none;
    }
    .collapsible-header:hover {
      background-color: #e9ecef;
    }
    .toggle-icon {
      display: inline-block;
      width: 18px;
      height: 18px;
      margin-right: 10px;
      transition: transform 0.2s;
      font-size: 1.1rem;
      color: #0d6efd;
      text-align: center;
      line-height: 18px;
    }
    .toggle-icon.collapsed {
      transform: rotate(-90deg);
      color: #6c757d;
    }
    .image-container {
      position: relative;
      margin: 0 auto 2rem auto;
      display: block;
      background: #fff;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
      border: 1px solid #dee2e6;
      border-top: none;
      max-width: 700px;
      overflow: hidden;
      transition: box-shadow 0.15s;
    }
    .camera-feed {
      display: block;
      margin: 0 auto;
      width: 100%;
      background: #e9ecef;
    }
    .hidden {
      display: none !important;
    }
    .json-container {
      max-width: 700px;
      margin: 0 auto 2rem auto;
      background-color: #f8f9fa;
      border: 1px solid #dee2e6;
      border-top: none;
      overflow: auto;
      max-height: 320px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
      font-size: 1rem;
      transition: box-shadow 0.15s;
    }
    #json-display {
      padding: 1rem;
      text-align: left;
      white-space: pre-wrap;
      font-family: 'Fira Mono', 'Consolas', monospace;
      font-size: 0.98rem;
      margin: 0;
      color: #495057;
      background: transparent;
    }
    .view-toggle {
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 0 auto 1rem auto;
      max-width: 700px;
      background: #fff;
      padding: 0.5rem 1rem;
      border: 1px solid #dee2e6;
      border-top: none;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    .view-toggle .form-check {
      margin-right: 1rem;
    }
    @media (max-width: 800px) {
      .image-container, .json-container, .collapsible-header, .view-toggle {
        max-width: 98vw;
      }
      #hi {
        max-height: 120px;
      }
    }
  </style>
</head>
<body>
  <img src="https://team315.org/imgs/00001.png" id="hi" alt="Team 3151A Banner">
  <h1 class="dashboard-title">Team 3151A Web Dashboard</h1>

  <div class="collapsible-header" id="feed-header">
    <span class="toggle-icon">▼</span> Camera Feed
  </div>

  <div class="view-toggle">
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="colorView" value="color" checked>
      <label class="form-check-label" for="colorView">Color Image</label>
    </div>
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="depthView" value="depth">
      <label class="form-check-label" for="depthView">Depth Map</label>
    </div>
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="noImageView" value="none">
      <label class="form-check-label" for="noImageView">No Image</label>
    </div>
  </div>

  <div class="image-container" id="container">
    <img id="camera-feed" class="camera-feed" src="/stream.mjpg" alt="Live Stream">
  </div>

  <div class="collapsible-header" id="json-header">
    <span class="toggle-icon collapsed">►</span> Raw JSON Data
  </div>
  <div class="json-container hidden" id="json-container">
    <pre id="json-display">Waiting for data…</pre>
  </div>

  <script>
    // Elements
    const feedHeader = document.getElementById('feed-header');
    const jsonHeader = document.getElementById('json-header');
    const container  = document.getElementById('container');
    const jsonCont   = document.getElementById('json-container');
    const toggleFeed = feedHeader.querySelector('.toggle-icon');
    const toggleJson = jsonHeader.querySelector('.toggle-icon');
    const cameraFeed = document.getElementById('camera-feed');
    const colorView  = document.getElementById('colorView');
    const depthView  = document.getElementById('depthView');
    const noneView   = document.getElementById('noImageView');
    const jsonDisplay= document.getElementById('json-display');

    // Collapse logic
    let feedVisible = true;
    feedHeader.addEventListener('click', () => {
      feedVisible = !feedVisible;
      container.classList.toggle('hidden', !feedVisible);
      toggleFeed.classList.toggle('collapsed', !feedVisible);
      toggleFeed.textContent = feedVisible ? '▼' : '►';
    });

    let jsonVisible = false;
    jsonHeader.addEventListener('click', () => {
      jsonVisible = !jsonVisible;
      jsonCont.classList.toggle('hidden', !jsonVisible);
      toggleJson.classList.toggle('collapsed', !jsonVisible);
      toggleJson.textContent = jsonVisible ? '▼' : '►';
    });

    // View‐mode logic (just shows or hides the MJPEG for now)
    function refreshView() {
      if (colorView.checked) {
        cameraFeed.src = '/color.mjpg';
        cameraFeed.classList.remove('hidden');
      } else if (depthView.checked) {
        cameraFeed.src = '/depth.mjpg';
        cameraFeed.classList.remove('hidden');
      } else {
        cameraFeed.src = '/no.mjpg';
        cameraFeed.classList.remove('hidden');
      }
    }
    [colorView, depthView, noneView].forEach(radio => {
      radio.addEventListener('change', refreshView);
    });

    // SSE for JSON
    const evt = new EventSource('/events');
    evt.onmessage = e => {
      try {
        const data = JSON.parse(e.data);
        jsonDisplay.textContent = JSON.stringify(data, null, 2);
      } catch(err) {
        console.error('Bad JSON:', e.data);
      }
    };
    evt.onerror = () => {
      console.warn('SSE connection error');
    };
  </script>
</body>
</html>
```



New code:

AIWorlds/comp3
__pycache__

```py AIWorlds/comp3/camera.py
try:
    import pyrealsense2 as rs
    _ = rs.pipeline()
except AttributeError:
    import pyrealsense2.pyrealsense2 as rs
    _ = rs.pipeline()
except Exception:
    print("Error importing pyrealsense2")
    exit(1)
import cv2
import numpy as np

class Camera:
    def __init__(self):
        self.pipeline = rs.pipeline()  # Initialize RealSense pipeline
        self.config = rs.config()
        # Enable depth stream at 640x480 in z16 encoding at 30fps
        self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        # Enable color stream at 640x480 in rgb8 encoding at 30fps
        self.config.enable_stream(
            rs.stream.color, 640, 480, rs.format.bgr8, 30)

    def start(self):
        self.profile = self.pipeline.start(self.config)  # Start the pipeline
        # Obtain depth sensor and calculate depth scale
        depth_sensor = self.profile.get_device().first_depth_sensor()
        self.depth_scale = depth_sensor.get_depth_scale()

    def get_frames(self):
        return self.pipeline.wait_for_frames()

    def stop(self):
        self.pipeline.stop()  # Stop the pipeline when finished

class Processing:
    def __init__(self, depth_scale):
        self.depth_scale = depth_scale
        self.align_to = rs.stream.color
        self.align = rs.align(self.align_to)
        self.HUE = 1.2
        self.SATURATION = 1.2
        self.VALUE = 0.8

    # checked
    def process_image(self, image):
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        hsv[..., 0] = hsv[..., 0] + self.HUE
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * self.SATURATION, 0, 255)
        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * self.VALUE, 0, 255)
        return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)

    # checked
    def align_frames(self, frames):
        # Align depth frames to color frames
        aligned_frames = self.align.process(frames)
        # Get the aligned frames and validate them
        self.depth_frame_aligned = aligned_frames.get_depth_frame()
        self.color_frame_aligned = aligned_frames.get_color_frame()

        if not self.depth_frame_aligned or not self.color_frame_aligned:
            self.depth_frame_aligned = None
            self.color_frame_aligned = None

    # checked
    def process_frames(self, frames):
        # Apply a color map to the depth image
        self.align_frames(frames)
        depth_image = np.asanyarray(self.depth_frame_aligned.get_data())
        # feed-forward factor, since it is always 3% off
        depth_image = depth_image / 1.03
        color_image = np.asanyarray(self.color_frame_aligned.get_data())
        # apply color correction to image
        color_image = self.process_image(color_image)
        depthImage = cv2.normalize(
            depth_image, None, alpha=0.01, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)

        return color_image, depth_image

# lightweight wrapper class to suit the Worker
class CameraWorker:
    def __init__(self):
        self._camera = Camera()
        self._camera.start()
        self._processing = Processing(self._camera.depth_scale)
        # color, depth
        self.frames = (
            np.zeros((480, 640, 3), dtype=np.uint8),
            np.zeros((480, 640), dtype=np.uint8)
        )

    def worker(self):
        try:
            while True:
                frames = self._camera.get_frames()
                self.frames = self._processing.process_frames(frames)
        finally:
            self._camera.stop()
    # snippet for getting color map from image:
    # depth_map = cv2.applyColorMap(depthImage, cv2.COLORMAP_JET)
    def close(self):
        self._camera.stop()
```

```py AIWorlds/comp3/constants.py
ENGINE_PATH = "/home/aadish/AIWorlds/comp2/yolov5n-best.engine"
MEASUREMENT_ROW = 240 # center row of image (image is 640w, 480h)
```

```py AIWorlds/comp3/inference.py
import time

import cv2
import numpy as np
import pycuda.driver as cuda
import tensorrt as trt
import torch
import torchvision as vision

from constants import ENGINE_PATH

class InferenceEngine:
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

    def __init__(
        self,
        engine_path: str,
        nms_params={
            "conf_thres": 0.20,
            "iou_thres": 0.5,
            "max_det": 50,
            "classes": ["blue", "goal", "red", "bot"],
        },
        input_shape=(1, 3, 640, 640),
        output_shape=(1, 25200, 9),
        device_id=0,
    ):
        # data
        self.engine_path = engine_path
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.nms_params = nms_params
        # set up CUDA device
        cuda.init()
        self.device = cuda.Device(device_id)
        # make_context() pushes the new context on the stack
        self.cuda_ctx = self.device.make_context()

        with open(self.engine_path, "rb") as f, trt.Runtime(self.TRT_LOGGER) as runtime:
            self.engine = runtime.deserialize_cuda_engine(f.read())
        if self.engine is None:
            raise SystemExit(f"Failed to load engine '{engine_path}'")
        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers(
            self.engine
        )
        self.trt_context = self.engine.create_execution_context()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    # MARK: - Inference utilities
    def _allocate_buffers(self, engine):
        inputs, outputs, bindings = [], [], []
        stream = cuda.Stream()
        for binding in engine:
            size = trt.volume(engine.get_binding_shape(
                binding)) * engine.max_batch_size
            dtype = trt.nptype(engine.get_binding_dtype(binding))

            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            bindings.append(int(device_mem))

            if engine.binding_is_input(binding):
                inputs.append({"host": host_mem, "device": device_mem})
            else:
                outputs.append({"host": host_mem, "device": device_mem})

        return inputs, outputs, bindings, stream

    def _do_preprocessing(self, img):
        img = cv2.resize(img, (self.input_shape[3], self.input_shape[2]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.astype(np.float32) / 255.0
        img = img.transpose((2, 0, 1))[None, ...]  # NHWC->NCHW
        img = np.ascontiguousarray(img)

        # Copy to host buffer
        np.copyto(self.inputs[0]["host"], img.ravel())

    def _do_inference(self):
        for inp in self.inputs:
            cuda.memcpy_htod_async(inp["device"], inp["host"], self.stream)
        self.trt_context.execute_async_v2(
            self.bindings, stream_handle=self.stream.handle
        )
        for out in self.outputs:
            cuda.memcpy_dtoh_async(out["host"], out["device"], self.stream)
        self.stream.synchronize()
        return [out["host"] for out in self.outputs]

    def close(self):
        # 1) sync & free your PyCUDA streams/bindings
        self.stream.synchronize()
        self.stream = None
        for entry in (self.inputs or []) + (self.outputs or []):
            entry["device"].free()
        self.inputs = None
        self.outputs = None

        # 2) explicitly drop TRT objects while context is still current
        #    this will invoke their __del__ (and hence free any TRT internal
        #    allocations) under the proper context
        del self.trt_context
        del self.engine
        self.trt_context = None
        self.engine = None

        # 3) now it’s safe to pop the context
        self.cuda_ctx.pop()
        self.cuda_ctx = None

    # MARK: - NMS utilies
    def _xywh2xyxy(self, boxes):
        # boxes: Tensor[M,4] = [xc,yc,w,h]
        x_c, y_c, w, h = boxes.unbind(1)
        return torch.stack((x_c - w / 2, y_c - h / 2, x_c + w / 2, y_c + h / 2), dim=1)

    @torch.no_grad()
    def _cpu_nms(
        self,
        pred: torch.Tensor,
        conf_thres: float = None,
        iou_thres: float = None,
        max_det: int = None,
    ) -> torch.Tensor:
        if conf_thres is None:
            conf_thres = self.nms_params["conf_thres"]
        if iou_thres is None:
            iou_thres = self.nms_params["iou_thres"]
        if max_det is None:
            max_det = self.nms_params["max_det"]
        """
        pred: Tensor[1, N, 5+nc] = [xc, yc, w, h, obj_conf, cls_conf_0, ..., cls_conf_nc-1]
        returns: Tensor[K,6] = [x1,y1,x2,y2,score,cls_idx]
        """
        x = pred.squeeze(0)  # [N,5+nc]
        if x.numel() == 0:
            return x.new_zeros((0, 6))

        # 1) objectness × best class confidence
        obj_conf = x[:, 4]  # [N]
        cls_conf_vals, cls_idx = x[:, 5:].max(dim=1)  # both [N]
        scores = obj_conf * cls_conf_vals  # [N]

        # 2) threshold
        mask = scores > conf_thres
        if not mask.any():
            return x.new_zeros((0, 6))

        x = x[mask]
        scores = scores[mask]
        cls_idx = cls_idx[mask]

        # 3) to corner format
        boxes = self._xywh2xyxy(x[:, :4])  # [M,4]

        keep = vision.ops.nms(boxes, scores, iou_thres)
        if keep.numel() > max_det:
            keep = keep[:max_det]

        # 5) gather detections
        det = torch.cat(
            (
                boxes[keep],
                scores[keep].unsqueeze(1),
                cls_idx[keep].unsqueeze(1).float(),
            ),
            dim=1,
        )  # [K,6]

        return det

    def _do_nms(self, out3d):
        det = self._cpu_nms(torch.from_numpy(out3d))
        if det.numel() == 0:
            return None
        det = det.numpy()  # [[x1,y1,x2,y2,score,cls], ...]

        detections = []
        for x1, y1, x2, y2, conf, cls in det:
            d = {
                "x": float((x1 + x2) / 2),
                "y": float((y1 + y2) / 2),
                "width": float(x2 - x1),
                "height": float(y2 - y1),
                "class": self.nms_params["classes"][int(cls)],
                "depth": float(0),
                "confidence": float(conf),
            }
            detections.append(d)
        return detections

    # MARK: - main entry point
    def run(self, img):
        # every frame:
        # preprocessing -> input -> tensorrt -> output -> postprocessing (nms) -> json blob
        self._do_preprocessing(img)
        trt_outs = self._do_inference()

        raw = trt_outs[0]
        try:
            out3d = raw.reshape(self.output_shape)
        except ValueError as e:
            print(
                f"ERROR: cannot reshape {raw.size} → {self.output_shape}: {e}")
            return []
        return self._do_nms(out3d) or []

class InferenceWorker:
    def __init__(self, app):
        self.camera = app.camera
        self.raw_detections = []
        self.engine = InferenceEngine(ENGINE_PATH)
        # self.camera.frames[0] is the most up-to-date rgb image
    def worker(self):
        self.engine.cuda_ctx.push()
        try:
            while True:
                img = self.camera.frames[0]
                self.raw_detections = self.engine.run(img)
                time.sleep(0.01)
        finally:
            # pop when you exit, so you don’t leak
            self.engine.cuda_ctx.pop()
            self.engine.close()
```

```py AIWorlds/comp3/main.py
from camera import CameraWorker
from inference import InferenceWorker
from post import Processing
import threading
import time
import signal
import logging
from flask import Flask, Response, stream_with_context
from flask_cors import CORS
import numpy as np
import cv2
import json
import atexit

class App:
    def __init__(self):
        print("test1")
        self.camera = CameraWorker()
        self.inference = InferenceWorker(self)
        self.most_recent_result = {}

    def service_simulator(self):
        print('test2')
        self.post = Processing(self, (0, 0, 0))
        while True:
            try:
                self.most_recent_result = self.post.update(5)
                print(self.most_recent_result)
            except Exception as e:
                print(f"Error in service_simulator: {e}")
            time.sleep(1.0 / 30.0)

    def run_dashboard(self):
        # stop excessive logging of requests
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.ERROR)

        app = Flask("3151App")
        CORS(app, resources={r"/*": {"origins": "*"}})

        @app.route('/<type>.mjpg')
        def mjpeg_stream(type):
            """
            MJPEG stream of color frames with overlaid detections.
            """
            def generate():
                boundary = b'--frame'
                while True:
                    color_img, depth_img = self.camera.frames
                    # Convert the raw depth (float or uint16) → 0–255 uint8
                    depth_8u = cv2.normalize(
                        depth_img, None,
                        alpha=0, beta=255,
                        norm_type=cv2.NORM_MINMAX,
                        dtype=cv2.CV_8U
                    )
                    depth_map = cv2.applyColorMap(depth_8u, cv2.COLORMAP_JET)
                    img = (color_img if type == 'color' else depth_map if type ==
                           'depth' else np.zeros_like(color_img)).copy()

                    # Draw detections onto img
                    for d in self.most_recent_result['stuff']:
                        x, y = d['x'], d['y']
                        w, h = d['width'], d['height']
                        cls = d['class']
                        conf = d['confidence']
                        depth = d.get('depth', None)

                        # top‑left corner
                        x0 = int(x - w/2)
                        y0 = int(y - h/2)
                        x1 = int(x + w/2)
                        y1 = int(y + h/2)

                        # choose a color
                        color = (0, 0, 255)
                        if cls == 'blue':
                            color = (255, 0, 0)
                        elif cls == 'goal':
                            color = (0, 215, 255)
                        elif cls == 'red':
                            color = (0, 0, 255)
                        elif cls == 'bot':
                            color = (0, 0, 0)

                        # rectangle
                        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)

                        # label text
                        label = f"{cls} {conf:.2f}"
                        if depth is not None and depth >= 0:
                            label += f" d={depth:.2f}m"
                        # putText above box
                        t_w, t_h = cv2.getTextSize(
                            label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                        txt_y = y0 - 5 if y0 - 5 > 10 else y0 + t_h + 5
                        cv2.rectangle(img, (x0, txt_y - t_h - 4),
                                      (x0 + t_w + 4, txt_y + 2), color, -1)
                        cv2.putText(
                            img, label, (x0+2, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                    # JPEG‐encode
                    success, jpg = cv2.imencode(
                        '.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
                    if not success:
                        continue
                    frame = jpg.tobytes()

                    # yield multipart chunk
                    yield boundary + b'\r\n' \
                        b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n'

                    # throttle to ~30fps
                    time.sleep(1/100)

            return Response(
                stream_with_context(generate()),
                mimetype='multipart/x-mixed-replace; boundary=frame'
            )

        @app.route('/events')
        def sse_events():
            """
            Server‑Sent Events stream of raw detection JSON.
            """
            def generate():
                while True:
                    data = self.most_recent_result
                    payload = f"data: {json.dumps(data)}\n\n"
                    yield payload
                    time.sleep(1/30)

            headers = {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
            return Response(stream_with_context(generate()), headers=headers)

        # A simple landing page to consume them
        @app.route('/')
        def index():
            with open('visualization.html', 'r') as f:
                return f.read()
        # Run Flask
        app.run(host='0.0.0.0', port=5000, threaded=True, debug=False)

    def close(self):
        self.camera.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

if __name__ != "main":
    exit()
with App() as app:
    try:
        threads = []
        t1 = threading.Thread(target=app.camera.worker, daemon=True)
        threads.append(t1)
        t2 = threading.Thread(target=app.inference.worker, daemon=True)
        threads.append(t2)
        t3 = threading.Thread(target=app.service_simulator, daemon=True)
        threads.append(t3)
        t4 = threading.Thread(target=app.run_dashboard, daemon=True)
        threads.append(t4)
        # all of the crazy lifecycle management stuff
        shutdown = threading.Event()

        def handle_sigterm(signum, frame):
            shutdown.set()

        signal.signal(signal.SIGINT, handle_sigterm)
        signal.signal(signal.SIGTERM, handle_sigterm)

        atexit.register(app.close)

        for t in threads:
            t.start()

        shutdown.wait()

        for t in threads:
            t.join(timeout=1)
            t.join(timeout=1)
    finally:
        app.close()
```

```py AIWorlds/comp3/post.py
import numpy as np
from constants import MEASUREMENT_ROW
import json
import copy
# oliver do your ~~thing~~ localization :P


class Localization:
    def __init__(self, initial_pose):
        self.pose = initial_pose  # (x, y, θ)

    def update(self, measurement, imu):
        # MARK: - THIS PART OF THE CODE WRITTEN BY OLIVER

        # imu is the imu heading in centidegrees (from 0 to 360*100)
        # measurement is a numpy array of shape (640,) representing a row of depth measurements in meters
        # use imu and measurement to update self.pose
        # this function needs to run in less than 1/30 of a second
        pass

# a post-processing class for detections and ...other stuff..., designed to be used with an EventDrivenSerial-like interface


class Processing:
    def __init__(self, app, pose):  # pose = (x, y, theta)
        self.app = app
        self.localization = Localization(pose)
        self.detections = []
        self.depth_scale = app.camera._camera.depth_scale
    def get_depth(self, detection, depth_img):
        try:
            height = detection["height"]
            width = detection["width"]
            top = max(detection["y"] - height / 2, 0)
            bottom = min(detection["y"] + height / 2, depth_img.shape[0])
            left = max(detection["x"] - width / 2, 0)
            right = min(detection["x"] + width / 2, depth_img.shape[1])
            depth_img = depth_img[int(top):int(
                bottom), int(left):int(right)].astype(float).flatten()
            p = np.percentile(depth_img[depth_img > 0], 10) * self.depth_scale
            return p
        except Exception:
            return -1
    def update(self, theta):  # update with new theta
        depth_image = self.app.camera.frames[1]
        # process detections
        detections = copy.deepcopy(self.app.inference.raw_detections)
        for detection in detections:
            # 480, 640
            detection['y'] *= 480 / 640
            depth_value = self.get_depth(
                detection, depth_image)
            if np.isnan(depth_value):
                detection['depth'] = -1
            else:
                detection['depth'] = depth_value
        self.detections = [
            i for i in detections if
            i['class'] not in ['red', 'blue'] or
            i['confidence'] > 0.6
        ]
        # run localization
        measurement = self.app.camera.frames[1][MEASUREMENT_ROW]
        self.localization.update(measurement, theta)
        # do collision detection
        flag = ""
        if (measurement[measurement > 0].shape[0] > 0):
            percentile = np.percentile(
                measurement[measurement > 0], 10) * self.depth_scale
            if percentile * 3.28 < 1:  # that is, at least 20% of nonzero readings are less than 1 foot away
                print("collision!!!!")
                flag = "STOP"
        # TODO: pose conversion

        # return all results
        output = {
            'pose': {
                'x': self.localization.pose[0],
                'y': self.localization.pose[1],
                'theta': self.localization.pose[2]
            },
            'stuff': self.detections,
            'flag': flag
        }
        return output
        # json.dumps(output, indent=None, separators=(',', ':'))
```

```html AIWorlds/comp3/visualization.html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>3151A Web Dashboard</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    html, body {
      margin: 0;
      padding: 0;
      background: #f8fafc;
      min-height: 100vh;
    }
    body {
      font-family: Arial, sans-serif;
      text-align: center;
    }
    #hi {
      height: 5vh;
      object-fit: cover;
      object-position: center;
      margin-bottom: 0.5rem;
      border-bottom: 2px solid #dee2e6;
    }
    .dashboard-title {
      font-weight: 700;
      letter-spacing: 1px;
      margin-bottom: 1.5rem;
      color: #212529;
      text-shadow: 0 1px 0 #fff, 0 2px 4px rgba(0,0,0,0.04);
    }
    .collapsible-header {
      display: flex;
      align-items: center;
      justify-content: flex-start;
      cursor: pointer;
      padding: 0.75rem 1.25rem;
      background-color: #f1f3f5;
      margin: 0 auto;
      max-width: 700px;
      font-size: 1.15rem;
      font-weight: 500;
      border: 1px solid #dee2e6;
      border-bottom: none;
      transition: background 0.15s;
      box-shadow: 0 1px 4px rgba(0,0,0,0.03);
      user-select: none;
    }
    .collapsible-header:hover {
      background-color: #e9ecef;
    }
    .toggle-icon {
      display: inline-block;
      width: 18px;
      height: 18px;
      margin-right: 10px;
      transition: transform 0.2s;
      font-size: 1.1rem;
      color: #0d6efd;
      text-align: center;
      line-height: 18px;
    }
    .toggle-icon.collapsed {
      transform: rotate(-90deg);
      color: #6c757d;
    }
    .image-container {
      position: relative;
      margin: 0 auto 2rem auto;
      display: block;
      background: #fff;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
      border: 1px solid #dee2e6;
      border-top: none;
      max-width: 700px;
      overflow: hidden;
      transition: box-shadow 0.15s;
    }
    .camera-feed {
      display: block;
      margin: 0 auto;
      width: 100%;
      background: #e9ecef;
    }
    .hidden {
      display: none !important;
    }
    .json-container {
      max-width: 700px;
      margin: 0 auto 2rem auto;
      background-color: #f8f9fa;
      border: 1px solid #dee2e6;
      border-top: none;
      overflow: auto;
      max-height: 320px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
      font-size: 1rem;
      transition: box-shadow 0.15s;
    }
    #json-display {
      padding: 1rem;
      text-align: left;
      white-space: pre-wrap;
      font-family: 'Fira Mono', 'Consolas', monospace;
      font-size: 0.98rem;
      margin: 0;
      color: #495057;
      background: transparent;
    }
    .view-toggle {
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 0 auto 1rem auto;
      max-width: 700px;
      background: #fff;
      padding: 0.5rem 1rem;
      border: 1px solid #dee2e6;
      border-top: none;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    .view-toggle .form-check {
      margin-right: 1rem;
    }
    @media (max-width: 800px) {
      .image-container, .json-container, .collapsible-header, .view-toggle {
        max-width: 98vw;
      }
      #hi {
        max-height: 120px;
      }
    }
  </style>
</head>
<body>
  <img src="https://team315.org/imgs/00001.png" id="hi" alt="Team 3151A Banner">
  <h1 class="dashboard-title">Team 3151A Web Dashboard</h1>

  <div class="collapsible-header" id="feed-header">
    <span class="toggle-icon">▼</span> Camera Feed
  </div>

  <div class="view-toggle">
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="colorView" value="color" checked>
      <label class="form-check-label" for="colorView">Color Image</label>
    </div>
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="depthView" value="depth">
      <label class="form-check-label" for="depthView">Depth Map</label>
    </div>
    <div class="form-check form-check-inline">
      <input class="form-check-input" type="radio" name="viewMode" id="noImageView" value="none">
      <label class="form-check-label" for="noImageView">No Image</label>
    </div>
  </div>

  <div class="image-container" id="container">
    <img id="camera-feed" class="camera-feed" src="/stream.mjpg" alt="Live Stream">
  </div>

  <div class="collapsible-header" id="json-header">
    <span class="toggle-icon collapsed">►</span> Raw JSON Data
  </div>
  <div class="json-container hidden" id="json-container">
    <pre id="json-display">Waiting for data…</pre>
  </div>

  <script>
    // Elements
    const feedHeader = document.getElementById('feed-header');
    const jsonHeader = document.getElementById('json-header');
    const container  = document.getElementById('container');
    const jsonCont   = document.getElementById('json-container');
    const toggleFeed = feedHeader.querySelector('.toggle-icon');
    const toggleJson = jsonHeader.querySelector('.toggle-icon');
    const cameraFeed = document.getElementById('camera-feed');
    const colorView  = document.getElementById('colorView');
    const depthView  = document.getElementById('depthView');
    const noneView   = document.getElementById('noImageView');
    const jsonDisplay= document.getElementById('json-display');

    // Collapse logic
    let feedVisible = true;
    feedHeader.addEventListener('click', () => {
      feedVisible = !feedVisible;
      container.classList.toggle('hidden', !feedVisible);
      toggleFeed.classList.toggle('collapsed', !feedVisible);
      toggleFeed.textContent = feedVisible ? '▼' : '►';
    });

    let jsonVisible = false;
    jsonHeader.addEventListener('click', () => {
      jsonVisible = !jsonVisible;
      jsonCont.classList.toggle('hidden', !jsonVisible);
      toggleJson.classList.toggle('collapsed', !jsonVisible);
      toggleJson.textContent = jsonVisible ? '▼' : '►';
    });

    // View‐mode logic (just shows or hides the MJPEG for now)
    function refreshView() {
      if (colorView.checked) {
        cameraFeed.src = '/color.mjpg';
        cameraFeed.classList.remove('hidden');
      } else if (depthView.checked) {
        cameraFeed.src = '/depth.mjpg';
        cameraFeed.classList.remove('hidden');
      } else {
        cameraFeed.src = '/no.mjpg';
        cameraFeed.classList.remove('hidden');
      }
    }
    [colorView, depthView, noneView].forEach(radio => {
      radio.addEventListener('change', refreshView);
    });

    // SSE for JSON
    const evt = new EventSource('/events');
    evt.onmessage = e => {
      try {
        const data = JSON.parse(e.data);
        jsonDisplay.textContent = JSON.stringify(data, null, 2);
      } catch(err) {
        console.error('Bad JSON:', e.data);
      }
    };
    evt.onerror = () => {
      console.warn('SSE connection error');
    };
  </script>
</body>
</html>
```



I need the ability to stream video to an endpoint. On my old code I used an MJPEG stream but that topped out at 6fps (even if I removed bounding box drawing). What is the fastest possible option here? I want it to be video call-level speed (it can definitely go to 30fps since something like Google Meet can do 30fps with much higher resolutions).

MJPEG over HTTP is about as far as you can push pure‑Python + JPEG in a multipart stream – you’re basically bound by:

  1. the cost of JPEG encode in user‑space (even libjpeg‑turbo will only run you up to ~20 – 30 fps at VGA resolution if you’re lucky, and only if you batch it or move it off the main thread), and
  2. the cost of Python doing socket writes and Flask marshalling.

If you want true “video‐call” speeds (30 fps+ at a few hundred kilobytes per frame) with sub‑200 ms latency, you need to switch to a video codec + transport built for real‑time:

1) Hardware‑accelerated H.264 (or VP8/VP9/H.265)
   • On Jetson you already have NVENC available.  Encode your 640×480 stream in H.264 with the “ultrafast” preset and “zerolatency” tune.
   • This cuts your per‑frame size by 10×–20× versus JPEG and offloads almost all the work to the hardware encoder.

2) A real‑time transport:
   • WebRTC (best browser support, sub‑100 ms end‑to‑end).
     – In Python you can use aiortc (https://github.com/aiortc/aiortc) to publish your NVENC‑encoded frames to the browser.
   • Or RTP/RTSP (e.g. gstreamer with rtph264pay + udpsink) and consume with a WebRTC gateway or a native RTSP client.
   • Even a raw WebSocket + Fragmented MP4 (fMP4) via MediaSource Extensions is orders of magnitude lighter than MJPEG over HTTP.

Concrete paths forward
----------------------

Option A: aiortc + NVENC
  – Server side (Python):
    1. Capture frames from your RealSense.
    2. Hand them into NVIDIA’s Video Codec SDK (PyNvCodec) or GStreamer’s `nvh264enc` for ultrafast H.264.
    3. Wrap the H.264 NALs in an aiortc RTCPeerConnection and send to the browser.

  – Client side (browser): standard WebRTC `getUserMedia` style consumer, or minimal JavaScript to negotiate and render into a `<video>` element.

Option B: GStreamer RTSP server
  – Launch a tiny RTSP server on the Jetson:
      gst-launch-1.0 v4l2src ! video/x-raw,format=BGR ! videoconvert ! x264enc bitrate=500 speed-preset=ultrafast tune=zerolatency ! rtph264pay config-interval=1 name=pay0 pt=96
  – Point any RTSP‑capable player (VLC, ffplay, or a WebRTC gateway) at `rtsp://<jetson_ip>:8554/test`.

Option C: WebSocket + fMP4
  – Encode with H.264 (as above), wrap NAL units into fragmented MP4 (fMP4) segments in Python.
  – Push over a raw WebSocket.
  – In the browser feed the incoming segments into a `MediaSource`/`SourceBuffer` attached to a `<video>` tag.

Why this is faster
-------------------

 • H.264/VP8/VP9 yield much smaller bitstreams so you spend far less time in the network/python I/O.
 • Hardware encoders (NVENC on the Jetson) chew through 640×480@30fps in <10 ms/frame.
 • WebRTC/RTP is designed for <200 ms latency with frame pacing, congestion control, retransmit, and jitter buffering.

Summary
-------

If you really want 30 fps with sub‑200 ms latency in a browser, drop MJPEG over HTTP and go to a hardware‑accelerated codec + WebRTC (via aiortc) or a GStreamer RTSP/RTP pipeline.  That is exactly how video‑calling apps get smooth 30 fps over commodity networks.

I need the one that goes at least 15 fps and requires minimal amounts of code.

If you just want “at least 15 fps” in a browser with literally a handful of lines of Python and zero C‑level hacking, stop doing MJPEG and leverage a real video codec + a tiny HTTP endpoint + a tiny bit of client JavaScript.  The recipe below will give you 30 fps VGA (<200 KB/frame → hell of a lot smaller than JPEG), hardware‑accelerated on Jetson, and will play back in any Chrome/Firefox via flv.js:

— server.py (Python + Flask + FFmpeg) —
```python
from flask import Flask, Response
import subprocess
import cv2
import threading

app = Flask(__name__)

def gen():
    # spawn ffmpeg: read raw bgr24 frames on stdin, encode with NVENC as H.264 into FLV on stdout
    cmd = [
      'ffmpeg',
      '-f', 'rawvideo', '-pixel_format', 'bgr24',
      '-video_size','640x480', '-framerate','30', '-i','pipe:0',
      '-c:v','h264_nvenc','-preset','llhp','-tune','zerolatency',
      '-f','flv','pipe:1'
    ]
    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=0)
    cap = cv2.VideoCapture(0)   # or your RealSense pipeline

    def pump():
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            # write raw frame into ffmpeg stdin
            p.stdin.write(frame.tobytes())
        cap.release()
        p.stdin.close()

    threading.Thread(target=pump, daemon=True).start()

    # now stream out the encoded FLV chunks
    while True:
        chunk = p.stdout.read(4096)
        if not chunk:
            break
        yield chunk

@app.route('/live.flv')
def live():
    return Response(gen(), mimetype='video/x-flv')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, threaded=True)
```

That’s ~25 lines, uses your Jetson’s NVENC to encode 640×480@30 fps H.264 in hardware, and wraps it in an FLV container for easy HTTP streaming.

— index.html (client) —
```html
<!DOCTYPE html>
<html>
  <body>
    <video id="video" controls autoplay style="width:640px;height:480px"></video>
    <script src="https://cdn.jsdelivr.net/npm/flv.js@latest/dist/flv.min.js"></script>
    <script>
      if (flvjs.isSupported()) {
        const player = flvjs.createPlayer({
          type: 'flv',
          url: 'http://' + location.host + '/live.flv'
        });
        player.attachMediaElement(document.getElementById('video'));
        player.load();
        player.play();
      } else {
        document.body.innerHTML = 'FLV.js not supported';
      }
    </script>
  </body>
</html>
```

Why this is so much faster than MJPEG:

  • You’re using hardware H.264 (NVENC) at “ultrafast”/“zerolatency” — <5 ms/frame encode time on Jetson, and ~50 KB/frame instead of ~200 KB+ for JPEG.
  • FLV over HTTP is trivial for Flask to chunk out — very low Python overhead.
  • flv.js in the browser will decode in software at 30 fps easily on any modern CPU/GPU.

You end up with <100 ms glass‑to‑glass latency at 30 fps, and the entire server is ~25 lines of Python.

aadish@aadish-desktop:~/AIWorlds/comp3$ python3.6 server.py
 * Serving Flask app 'server' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://192.168.86.98:5000/ (Press CTRL+C to quit)
[ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (1757) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module v4l2src0 reported: Internal data stream error.
[ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (886) open OpenCV | GStreamer warning: unable to start pipeline
[ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created
VIDEOIO ERROR: V4L2: Pixel format of incoming image is unsupported by OpenCV
VIDEOIO ERROR: V4L: can't open camera by index 0
ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers
  built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)
  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared
  libavutil      55. 78.100 / 55. 78.100
  libavcodec     57.107.100 / 57.107.100
  libavformat    57. 83.100 / 57. 83.100
  libavdevice    57. 10.100 / 57. 10.100
  libavfilter     6.107.100 /  6.107.100
  libavresample   3.  7.  0 /  3.  7.  0
  libswscale      4.  8.100 /  4.  8.100
  libswresample   2.  9.100 /  2.  9.100
  libpostproc    54.  7.100 / 54.  7.100
Input #0, rawvideo, from 'pipe:0':
  Duration: N/A, bitrate: 221184 kb/s
    Stream #0:0: Video: rawvideo (BGR[24] / 0x18524742), bgr24, 640x480, 221184 kb/s, 30 tbr, 30 tbn, 30 tbc
Unknown encoder 'h264_nvenc'
192.168.86.118 - - [27/Apr/2025 23:26:46] "GET /live.flv HTTP/1.1" 200 -

On the Jetson your stock FFmpeg isn’t actually built with NVIDIA’s NVENC APIs, so “h264_nvenc” simply doesn’t exist.  You have two almost–zero‑code options:

1) Switch to the Jetson’s OpenMAX H.264 encoder:

   In your `server.py` change the FFmpeg command like this:
   ```python
   cmd = [
     'ffmpeg',
     '-f', 'rawvideo', '-pixel_format', 'bgr24',
     '-video_size',   '640x480', '-framerate', '30', '-i', 'pipe:0',
     '-c:v', 'h264_omx',             # ← use the OpenMAX hw encoder
     '-preset', 'ultrafast',         # minimal latency
     '-tune',   'zerolatency',
     '-f', 'flv', 'pipe:1'
   ]
   ```
   That one change will let FFmpeg offload encoding to the Jetson’s hardware block and keep your existing Flask+flv.js streaming exactly the same.  You should easily hit 20–30 fps.

2) If for some reason “h264_omx” also isn’t available, fall back to CPU x264 in the same minimal‑diff way:
   ```diff
   -  '-c:v','h264_nvenc',
   +  '-c:v','libx264',   # CPU encode
      '-preset','ultrafast',
      '-tune','zerolatency',
   ```
   On a Xavier/Xavier NX you’ll still get ~15–20 fps at 640×480.  (On a Nano it may be closer to 10–15 fps, but you asked for ≥ 15.)

If you want to verify what encoders your FFmpeg build has, just run:

```
ffmpeg -encoders | grep 264
```

and pick whichever hardware‑accelerated encoder shows up (e.g. `h264_omx`).

With that one‑line change you keep all your Python/Flask boilerplate, keep using flv.js in the browser, and immediately jump from 6 fps MJPEG to 20–30 fps H.264.

Did #1

aadish@aadish-desktop:~/AIWorlds/comp3$ python3.6 server.py
 * Serving Flask app 'server' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://192.168.86.98:5000/ (Press CTRL+C to quit)
ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers
  built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)
  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared
  libavutil      55. 78.100 / 55. 78.100
  libavcodec     57.107.100 / 57.107.100
  libavformat    57. 83.100 / 57. 83.100
  libavdevice    57. 10.100 / 57. 10.100
  libavfilter     6.107.100 /  6.107.100
  libavresample   3.  7.  0 /  3.  7.  0
  libswscale      4.  8.100 /  4.  8.100
  libswresample   2.  9.100 /  2.  9.100
  libpostproc    54.  7.100 / 54.  7.100
[ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1
Input #0, rawvideo, from 'pipe:0':
  Duration: N/A, start: 0.000000, bitrate: 221184 kb/s
    Stream #0:0: Video: rawvideo (BGR[24] / 0x18524742), bgr24, 640x480, 221184 kb/s, 30 tbr, 30 tbn, 30 tbc
Codec AVOption preset (Configuration preset) specified for output file #0 (pipe:1) has not been used for any stream. The most likely reason is either wrong type (e.g. a video option with no video streams) or that it is a private option of some encoder which was not actually used for any stream.
Codec AVOption tune (Tune the encoding to a specific scenario) specified for output file #0 (pipe:1) has not been used for any stream. The most likely reason is either wrong type (e.g. a video option with no video streams) or that it is a private option of some encoder which was not actually used for any stream.
Stream mapping:
  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (h264_omx))
[h264_omx @ 0x55b20a6060] libOMX_Core.so not found
[h264_omx @ 0x55b20a6060] libOmxCore.so not found
Error initializing output stream 0:0 -- Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height
Conversion failed!
192.168.86.118 - - [27/Apr/2025 23:28:35] "GET /live.flv HTTP/1.1" 200 -
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "server.py", line 30, in pump
    p.stdin.write(frame.tobytes())
BrokenPipeError: [Errno 32] Broken pipe


Most of the error you’re seeing is simply “there is no OMX hw‐encoder in your FFmpeg build,” so your “h264_omx” line never actually got used and FFmpeg immediately died.  You have two almost‑zero‑code paths to ≥15 fps:

1) Install the OMX encoder libraries so that `h264_omx` really exists.
   On Jetson (Ubuntu 18.04) that usually means:
     sudo apt install gstreamer1.0-omx libomxil-bellagio
   Then rerun your server.py with `-c:v h264_omx` and you’ll get true hardware encode at ~30 fps out of the box.

2) If you don’t want any extra installs at all, fall back to CPU‑x264.  Change your ffmpeg‐cmd from:
   `-c:v h264_omx …`
   to
   `-c:v libx264 -preset ultrafast -tune zerolatency -threads 0 …`
   That single swap will give you ~15–20 fps on Xavier/NX (even on a Nano you’ll hover around 10–15 fps).  If you still need more headroom, you can downsize to 320×240:

   ```python
   # in pump()
   frame = cv2.resize(frame, (320,240))
   p.stdin.write(frame.tobytes())
   ```
   and in your FFmpeg args change `-video_size 640x480` → `-video_size 320x240`.

Either approach is a one‑line tweak to your existing 25‑line Flask+FFmpeg+flv.js server.  You’ll immediately go from 6 fps MJPEG to 15–30 fps H.264 with zero change on the client (still just flv.js in the browser).

Got this output, but still seeing an endless loading spinner on the frontend.

* Serving Flask app 'server' (lazy loading)
* Environment: production
  WARNING: This is a development server. Do not use it in a production deployment.
  Use a production WSGI server instead.
* Debug mode: off
* Running on all addresses.
  WARNING: This is a development server. Do not use it in a production deployment.
* Running on http://192.168.86.112:5000/ (Press CTRL+C to quit)
ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers
 built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)
 configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared
 libavutil      55. 78.100 / 55. 78.100
 libavcodec     57.107.100 / 57.107.100
 libavformat    57. 83.100 / 57. 83.100
 libavdevice    57. 10.100 / 57. 10.100
 libavfilter     6.107.100 /  6.107.100
 libavresample   3.  7.  0 /  3.  7.  0
 libswscale      4.  8.100 /  4.  8.100
 libswresample   2.  9.100 /  2.  9.100
 libpostproc    54.  7.100 / 54.  7.100
[ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1
Input #0, rawvideo, from 'pipe:0':
 Duration: N/A, start: 0.000000, bitrate: 221184 kb/s
   Stream #0:0: Video: rawvideo (BGR[24] / 0x18524742), bgr24, 640x480, 221184 kb/s, 30 tbr, 30 tbn, 30 tbc
Stream mapping:
 Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))
[libx264 @ 0x5578d88060] using cpu capabilities: ARMv8 NEON
[libx264 @ 0x5578d88060] profile High 4:4:4 Predictive, level 3.0, 4:4:4 8-bit
[libx264 @ 0x5578d88060] 264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=0 ref=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=6 threads=4 lookahead_threads=4 sliced_threads=1 slices=4 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=25 scenecut=0 intra_refresh=0 rc=crf mbtree=0 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=0
Output #0, flv, to 'pipe:1':
 Metadata:
   encoder         : Lavf57.83.100
   Stream #0:0: Video: h264 (libx264) ([7][0][0][0] / 0x0007), yuv444p, 640x480, q=-1--1, 30 fps, 1k tbn, 30 tbc
   Metadata:
     encoder         : Lavc57.107.100 libx264
   Side data:
     cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
192.168.86.118 - - [27/Apr/2025 23:32:24] "GET /live.flv HTTP/1.1" 200 -
frame=   29 fps= 28 q=18.0 size=      29kB time=00:00:00.93 bitrate= 253.0kbits/s speed=0.914x

Two things turned out to be tripping us up:

  1) by default flv.js will treat an HTTP‑FLV feed as a VOD file (it will try to do HEAD and byte‑range requests) and never actually start playout;
  2) we were feeding it a “yuv444p, High‑profile” H.264 stream, which most browsers’ FLV demuxers choke on.

All you need to do is:

  • Tell flv.js “this is live” so it stops doing ranges and just plays the incoming chunks,
  • Force ffmpeg/x264 to output a 4:2:0 stream (pixel format yuv420p / baseline profile).

— server.py —
```diff
     cmd = [
       "ffmpeg",
       "-f",   "rawvideo",
       "-pixel_format","bgr24",
       "-video_size","640x480",
       "-framerate","30",
       "-i",   "pipe:0",
-      "-c:v","libx264",
-      "-preset","ultrafast",
-      "-tune","zerolatency",
+      "-c:v","libx264",
+      "-preset","ultrafast",
+      "-tune","zerolatency",
+      "-pix_fmt","yuv420p",         # <<< force 4:2:0
+      "-profile:v","baseline",      # <<< baseline profile for FLV compat
       "-f",   "flv",
       "pipe:1"
     ]
```

— index.html —
```html
  <video id="video" autoplay muted style="width:640px;height:480px"></video>
  <script src="https://cdn.jsdelivr.net/npm/flv.js@latest/dist/flv.min.js"></script>
  <script>
    if (flvjs.isSupported()) {
      const player = flvjs.createPlayer({
        type: 'flv',
        url:  'http://' + location.host + '/live.flv',
        isLive: true,      // <<< tell flv.js it’s a live feed
        cors:   true
      });
      player.attachMediaElement(document.getElementById('video'));
      player.load();
      player.play();
    } else {
      console.error('flv.js not supported');
    }
  </script>
```

That one‑line tweak in ffmpeg plus adding `isLive:true` in flv.js will instantly unlock 20–30 fps streaming over HTTP/FLV with zero extra C‑code or WebRTC setup.

```py AIWorlds/comp3/server.py
from flask import Flask, Response
import subprocess
import cv2
import threading

app = Flask(__name__)


def gen():
    # spawn ffmpeg: read raw bgr24 frames on stdin, encode with NVENC as H.264 into FLV on stdout
    cmd = [
        'ffmpeg',
        '-f', 'rawvideo', '-pixel_format', 'bgr24',
        '-video_size',   '640x480', '-framerate', '30', '-i', 'pipe:0',
        '-c:v', 'libx264',             # ← use the OpenMAX hw encoder
        '-preset', 'ultrafast',         # minimal latency
        '-tune',   'zerolatency',
        '-pix_fmt', 'yuv420p',
        '-profile:v', 'baseline',
        '-f', 'flv', 'pipe:1'
    ]
    p = subprocess.Popen(cmd, stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE, bufsize=0)
    cap = cv2.VideoCapture(2)   # or your RealSense pipeline

    def pump():
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            # write raw frame into ffmpeg stdin
            p.stdin.write(frame.tobytes())
        cap.release()
        p.stdin.close()

    threading.Thread(target=pump, daemon=True).start()

    # now stream out the encoded FLV chunks
    while True:
        chunk = p.stdout.read(4096)
        if not chunk:
            break
        yield chunk


@app.route('/live.flv')
def live():
    return Response(gen(), mimetype='video/x-flv')


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, threaded=True)
```


Can you add CORS? Tell me what to change (use a diff-fenced format).

Here’s a minimal patch using the flask‑cors extension. You’ll need to pip install flask‑cors if you haven’t already.

```diff
--- a/server.py
+++ b/server.py
@@
-from flask import Flask, Response
+from flask import Flask, Response
+from flask_cors import CORS
@@
-app = Flask(__name__)
+app = Flask(__name__)
+# enable CORS on all routes/origins
+CORS(app)
```

If you want to be extra explicit (only on your FLV endpoint), you can instead do:

```diff
--- a/server.py
+++ b/server.py
@@
 from flask import Flask, Response
 from flask_cors import CORS
@@
 app = Flask(__name__)
-CORS(app)
+cors = CORS(app, resources={
+    r"/live.flv": {"origins": "*"}
+})
```

That’s it – now your `/live.flv` responses will carry
`Access‑Control‑Allow‑Origin: *` (or whatever you configure).

Okay now it works but rly weirdly. The image has three copies side-by-side compressed horizontally (w/ some overlap). The motion comes in short bursts and then freezes, and then happens all over again.

What you’re seeing (three little 640 px‑wide “tiles” side by side, bursty updates, then a freeze) is exactly what happens when you feed FFmpeg raw bytes that don’t line up with your declared frame size (640×480×3 bytes).  In your case `cap.read()` is giving you something larger (e.g. 1920×480 or 1920×1080), so FFmpeg consumes the first 640×480 chunk as frame 1, then the next 640×480 as frame 2, then the next as frame 3, then runs out of data and waits for more.  Hence the triplets + bursty updates.

The fix is to force your OpenCV capture (or resize) to 640×480 exactly before you write into FFmpeg’s stdin.  Here’s a 5‑line diff that will solve it:

```diff
--- a/server.py
+++ b/server.py
@@ def gen():
-    cap = cv2.VideoCapture(2)   # or your RealSense pipeline
+    cap = cv2.VideoCapture(2)   # or your RealSense pipeline
+    # force capture to exactly 640×480 so rawvideo frame size matches
+    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)
+    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

@@ def pump():
-        ret, frame = cap.read()
+        ret, frame = cap.read()
         if not ret:
             break
+        # make absolutely sure we’re feeding FFmpeg 640×480×3 bytes
+        frame = cv2.resize(frame, (640, 480))
         # write raw frame into ffmpeg stdin
         p.stdin.write(frame.tobytes())
```

With those two `cap.set(...)` calls (or the explicit `resize`) you’ll be guaranteed that each `frame.tobytes()` is exactly 640 × 480 × 3 = 921 600 bytes, so FFmpeg will consume one frame at a time, and you’ll get a smooth 15–30 fps H.264/FLV stream again.

Great! It looks good now but the bursts are still happening. I'm accessing the color camera of my realsense (VideoCapture(3)) so maybe that changes smth?

That “3‑tile burst” pattern is almost always the encoder only emitting packets in GOP‑sized bursts (SPS/PPS + IDR + P‑frames) and otherwise buffering everything. We can force FFmpeg to:

  1. flush out each packet immediately,
  2. use no B‑frames,
  3. send SPS/PPS before every key‐frame,
  4. keep key‐frames frequent (every 30 frames).

Just drop these extra flags into your `cmd` list:

```diff
--- a/server.py
+++ b/server.py
@@ def gen():
-    cmd = [
+    cmd = [
+        'ffmpeg',
+        # no buffer, low‐delay, flush per packet:
+        '-fflags',      'nobuffer',
+        '-flags',       'low_delay',
+        '-flush_packets','1',
+        # raw input:
         'ffmpeg',
         '-f', 'rawvideo', '-pixel_format', 'bgr24',
         '-video_size',   '640x480', '-framerate', '30', '-i', 'pipe:0',
-        '-c:v', 'libx264',
-        '-preset', 'ultrafast',
-        '-tune',   'zerolatency',
+        # software H264 @ ultrafast/zerolatency:
+        '-c:v',       'libx264',
+        '-preset',    'ultrafast',
+        '-tune',      'zerolatency',
+        # ensure 4:2:0, baseline profile, no B‑frames:
+        '-pix_fmt',   'yuv420p',
+        '-profile:v', 'baseline',
+        '-g',         '30',       # key‑frame every 30 frames
+        '-bf',        '0',        # no B‑frames
+        # repeat SPS/PPS before each IDR:
+        '-x264-params','keyint=30:scenecut=0:repeat-headers=1',
         '-f', 'flv', 'pipe:1'
     ]
```

After that, restart your server. You should see a smooth ~15–30 fps feed instead of the triple‐burst freeze.

can you rewrite the entire `cmd` for this?

Absolutely! Here’s the **full `cmd` list** for your `subprocess.Popen` call, with all the recommended flags for smooth, low-latency, no-burst, browser-compatible H.264/FLV streaming:

```python
cmd = [
    'ffmpeg',
    '-fflags', 'nobuffer',
    '-flags', 'low_delay',
    '-flush_packets', '1',
    '-f', 'rawvideo',
    '-pixel_format', 'bgr24',
    '-video_size', '640x480',
    '-framerate', '30',
    '-i', 'pipe:0',
    '-c:v', 'libx264',
    '-preset', 'ultrafast',
    '-tune', 'zerolatency',
    '-pix_fmt', 'yuv420p',
    '-profile:v', 'baseline',
    '-g', '30',  # keyframe every 30 frames
    '-bf', '0',  # no B-frames
    '-x264-params', 'keyint=30:scenecut=0:repeat-headers=1',
    '-f', 'flv',
    'pipe:1'
]
```

**Just replace your old `cmd = [...]` with the above.**

---

**What these do:**
- `-fflags nobuffer -flags low_delay -flush_packets 1`: minimize latency and flush output ASAP.
- `-c:v libx264 -preset ultrafast -tune zerolatency`: fastest possible x264 encode.
- `-pix_fmt yuv420p -profile:v baseline`: browser/FLV compatibility.
- `-g 30 -bf 0 -x264-params keyint=30:scenecut=0:repeat-headers=1`: frequent keyframes, no B-frames, SPS/PPS before every keyframe.

---

**This should eliminate the bursty/freezing behavior and give you smooth, real-time streaming!**

Looks good but still seeing a massive delay (~3 seconds), is there any way to dramatically reduce this? I'd be okay with downscaling/reducing quality of video if that's needed

You’re bumping into three sources of latency:

 1. The encoder’s internal frame buffering (GOP buffering, packet queues)
 2. FFmpeg → HTTP chunking buffer
 3. flv.js’s “stash” / initial buffer before it starts playback

Below is a recipe that hits all three:

  • **Down‑scale** to 320×240 @ 15 fps
  • **Crank bitrate way down** so the network/Python I/O can keep up
  • **Force key‑frames every 15 frames**, no B‑frames, repeat SPS/PPS on every IDR
  • **Drop FFmpeg’s buffering flags** so it flushes every packet ASAP
  • **Tell flv.js it’s live** and trim its initial stash to just a few packets

— server.py —

```diff
--- a/server.py
+++ b/server.py
@@ def gen():
-    cmd = [
-        'ffmpeg',
-        '-fflags', 'nobuffer',
-        '-flags', 'low_delay',
-        '-flush_packets', '1',
-        '-f', 'rawvideo',
-        '-pixel_format', 'bgr24',
-        '-video_size', '640x480',
-        '-framerate', '30',
-        '-i', 'pipe:0',
-        '-c:v', 'libx264',
-        '-preset', 'ultrafast',
-        '-tune', 'zerolatency',
-        '-pix_fmt', 'yuv420p',
-        '-profile:v', 'baseline',
-        '-g', '30',
-        '-bf', '0',
-        '-x264-params', 'keyint=30:scenecut=0:repeat-headers=1',
-        '-f', 'flv', 'pipe:1'
-    ]
+    cmd = [
+        'ffmpeg',
+        # no internal buffering, flush immediately
+        '-fflags',       'nobuffer',
+        '-flags',        'low_delay',
+        '-flush_packets','1',
+
+        # input: 320×240 @ 15fps raw BGR
+        '-f',            'rawvideo',
+        '-pixel_format', 'bgr24',
+        '-video_size',   '320x240',
+        '-framerate',    '15',
+        '-i',            'pipe:0',
+
+        # ultrafast low‑latency x264 @ low bit‑rate
+        '-c:v',          'libx264',
+        '-preset',       'ultrafast',
+        '-tune',         'zerolatency',
+        '-b:v',          '300k',          # target 300 kbps
+        '-maxrate',      '300k',
+        '-bufsize',      '300k',
+
+        # browser‑friendly 4:2:0 baseline, every 15 frames a keyframe
+        '-pix_fmt',      'yuv420p',
+        '-profile:v',    'baseline',
+        '-g',            '15',
+        '-bf',           '0',
+        '-x264-params',  'keyint=15:scenecut=0:repeat-headers=1',
+
+        # output as HTTP‑FLV
+        '-f',            'flv',
+        'pipe:1'
+    ]
```

Make sure you also force your capture to 320×240 (either via `cap.set(...)` or an explicit `resize(frame,(320,240))` as shown previously).

— client (index.html) —

```diff
     <script src="https://cdn.jsdelivr.net/npm/flv.js@latest/dist/flv.min.js"></script>
     <script>
-      if (flvjs.isSupported()) {
-        const player = flvjs.createPlayer({
-          type: 'flv',
-          url:  'http://' + location.host + '/live.flv',
-          isLive: true,
-          cors:   true
-        });
+      if (flvjs.isSupported()) {
+        const player = flvjs.createPlayer({
+          type: 'flv',
+          url:  'http://' + location.host + '/live.flv',
+          isLive: true,
+          cors:   true,
+          stashInitialSize: 4   // chew just 4 packets before play
+        });
         player.attachMediaElement(document.getElementById('video'));
         player.load();
         player.play();
```

With this in place you should see sub‑500 ms end‑to‑end latency at ~15 fps. If you still need lower latency you can:

  • drop to 10 fps (`-framerate 10`)
  • lower resolution further
  • shave bitrate even more

—but chances are 320×240@15fps@300 kbps + ultra‑low buffers will feel nearly real‑time.

Still insanely slow. Random note that may help: decreasing the speed to 5fps (in the -framerate arg) actually increases the lag by ~5x.

Below are the two minimal tweaks you can make to rip out almost all of the remaining buffering and get down into the sub‑second range.

1) **Server**: tell FFmpeg to drop all “file‑length” metadata out of the FLV header so the player won’t sit around waiting for a duration/size, by adding `-flvflags no_duration_filesize`.

2) **Client**: tell flv.js to turn off its stash buffer entirely (`enableStashBuffer: false`) and only grab 1 packet before it starts playing (`stashInitialSize: 1`).

— server.py —
```diff
--- a/server.py
+++ b/server.py
@@ def gen():
-    cmd = [
+    cmd = [
         'ffmpeg',
         '-fflags',       'nobuffer',
         '-flags',        'low_delay',
         '-flush_packets','1',
@@
-        # output as HTTP‑FLV
-        '-f',            'flv',
-        'pipe:1'
+        # strip out duration/filesize so the client doesn't wait
+        '-flvflags',     'no_duration_filesize',
+        # output as HTTP‑FLV
+        '-f',            'flv',
+        'pipe:1'
     ]
```

— index.html —
```diff
--- a/visualization.html
+++ b/visualization.html
@@ <script>
-    if (flvjs.isSupported()) {
-      const player = flvjs.createPlayer({
-        type: 'flv',
-        url:  'http://' + location.host + '/live.flv',
-        isLive: true,
-        cors:   true,
-        stashInitialSize: 4   // chew just 4 packets before play
-      });
+    if (flvjs.isSupported()) {
+      const player = flvjs.createPlayer({
+        type: 'flv',
+        url:  'http://' + location.host + '/live.flv',
+        isLive: true,
+        cors:   true,
+        enableStashBuffer: false,  // turn off the big stash buffer
+        stashInitialSize: 1        // only wait 1 packet before play
+      });
       player.attachMediaElement(document.getElementById('video'));
       player.load();
       player.play();
```

After making those two tiny changes you should see your end‑to‑end latency collapse from multiple seconds down to well under one second.
